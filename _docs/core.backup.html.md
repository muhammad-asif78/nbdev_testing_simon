# **Downloading the Dataset and the weights of the pre trained model**


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

### **Packages Installation**

**Download the Dataset**

**Download the Weights of the RF-DETR**

``` python
!pip install --quiet gdown

import re
import subprocess
from pathlib import Path

def download_drive_to_weights(link_or_id):
    dest = "weights"
    Path(dest).mkdir(parents=True, exist_ok=True)
    print(f"Using '{dest}' folder for downloads...")

    s = str(link_or_id).strip()

    m = re.search(r'folders/([A-Za-z0-9_-]+)', s) or re.search(r'id=([A-Za-z0-9_-]+)', s)
    if not m:
        m = re.search(r'/d/([A-Za-z0-9_-]+)', s) or re.search(r'([A-Za-z0-9_-]{10,})$', s)
    if not m:
        print("Couldn't parse an ID from the input. Please paste the folder link or the long '1...' id.")
        return
    fid = m.group(1)

    use_folder = ('folders' in s) or (len(fid) > 20)

    try:
        if use_folder:
            print("Downloading Drive folder to ./weights ...")
            subprocess.run(
                ["gdown", "--folder", f"https://drive.google.com/drive/folders/{fid}", "-O", dest],
                check=True
            )
        else:
            print("Downloading file to ./weights ...")
            url = f"https://drive.google.com/uc?id={fid}"
            subprocess.run(
                ["gdown", url, "-O", dest],
                check=True
            )

        print("Done — check the local 'weights' folder.")
    except subprocess.CalledProcessError as e:
        print("gdown failed:", e)


link_or_id = "https://drive.google.com/drive/folders/17CXXFCGw5npR1cyrg082yq1IdcnH6-ms?usp=sharing"
download_drive_to_weights(link_or_id)
```

    Using 'weights' folder for downloads...
    Downloading Drive folder to ./weights ...
    Done — check the local 'weights' folder.

**Clone the Sam2 Model**

``` python
!git clone https://github.com/facebookresearch/sam2.git

!pip install -e .
```

    Cloning into 'sam2'...
    remote: Enumerating objects: 1070, done.
    remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)
    Receiving objects: 100% (1070/1070), 128.11 MiB | 11.61 MiB/s, done.
    Resolving deltas: 100% (380/380), done.
    Updating files: 100% (569/569), done.
    /content/sam2
    Obtaining file:///content/sam2
      Installing build dependencies ... done
      Checking if build backend supports build_editable ... done
      Getting requirements to build editable ... done
      Preparing editable metadata (pyproject.toml) ... done
    Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (2.9.0+cu126)
    Requirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (0.24.0+cu126)
    Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (2.0.2)
    Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (4.67.1)
    Collecting hydra-core>=1.3.2 (from SAM-2==1.0)
      Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)
    Collecting iopath>=0.1.10 (from SAM-2==1.0)
      Downloading iopath-0.1.10.tar.gz (42 kB)
         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 4.6 MB/s eta 0:00:00
      Preparing metadata (setup.py) ... done
    Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (11.3.0)
    Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)
    Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)
    Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (25.0)
    Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.15.0)
    Collecting portalocker (from iopath>=0.1.10->SAM-2==1.0)
      Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)
    Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.20.0)
    Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (75.2.0)
    Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.14.0)
    Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.6)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.6)
    Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2025.3.0)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.77)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.77)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.80)
    Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (9.10.2.21)
    Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.4.1)
    Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.3.0.4)
    Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (10.3.7.77)
    Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.7.1.2)
    Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.5.4.2)
    Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (0.7.1)
    Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2.27.5)
    Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.3.20)
    Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.77)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.6.85)
    Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.11.1.6)
    Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.5.0)
    Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.5.1->SAM-2==1.0) (1.3.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (3.0.3)
    Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 15.6 MB/s eta 0:00:00
    Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)
    Building wheels for collected packages: SAM-2, iopath
      Building editable for SAM-2 (pyproject.toml) ... done
      Created wheel for SAM-2: filename=sam_2-1.0-0.editable-cp312-cp312-linux_x86_64.whl size=13852 sha256=f87822b85d374b5db5dd1b43bd7b95cd61395b461bf2e45266cae9a65f542ff4
      Stored in directory: /tmp/pip-ephem-wheel-cache-hd59s1yu/wheels/9e/fa/17/14aaeb20d3ca07c58ee93742054d4479f89c243063ce0b61b9
      Building wheel for iopath (setup.py) ... done
      Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=650d2af9f6e79b48459227f4fe01035aac6dcc406f998403e66fe84cc26aabee
      Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c
    Successfully built SAM-2 iopath
    Installing collected packages: portalocker, iopath, hydra-core, SAM-2
    Successfully installed SAM-2-1.0 hydra-core-1.3.2 iopath-0.1.10 portalocker-3.2.0

``` python
```

    /content

**Downlaod the weights of angle models**

``` python
!pip install --quiet gdown

import re
import shutil
import os
import subprocess
import zipfile

def download_and_extract_strip_top_folder(link_or_id, dest="/content/angle-models-weights"):
    m = re.search(r'id=([A-Za-z0-9_-]+)', link_or_id) or re.search(r'/d/([A-Za-z0-9_-]+)', link_or_id)
    if not m:
        raise ValueError("Paste a valid Google Drive file link")
    fid = m.group(1)

    print("Downloading zip...")
    url = f"https://drive.google.com/uc?id={fid}"

    if os.path.exists(dest):
        shutil.rmtree(dest)
    os.makedirs(dest)

    subprocess.run(["gdown", url, "-O", "weights.zip"], check=True)

    print("Extracting zip file and stripping top-level folder...")

    with zipfile.ZipFile("weights.zip", 'r') as zip_ref:

        all_paths = zip_ref.namelist()
        top_level_dirs = set([p.split('/')[0] for p in all_paths if p.strip() != ''])
        if len(top_level_dirs) == 1:
            top_folder = list(top_level_dirs)[0]
        else:
            top_folder = None

        for member in all_paths:
            if top_folder:
                rel_path = member[len(top_folder)+1:] if member.startswith(top_folder + '/') else member
            else:
                rel_path = member

            if rel_path == '':

                continue

            dest_path = os.path.join(dest, rel_path)
            if member.endswith('/'):
                os.makedirs(dest_path, exist_ok=True)
            else:
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                with zip_ref.open(member) as source, open(dest_path, 'wb') as target:
                    shutil.copyfileobj(source, target)

    print(f"Done — check the local '{dest}' folder.")


link = "https://drive.google.com/file/d/1uI2GlE5JviTJccZxzLP5lQ6YiwjYTvVe/view?usp=sharing"
download_and_extract_strip_top_folder(link, dest="/content/angle-models-weights")
```

    Downloading zip...
    Extracting zip file and stripping top-level folder...
    Done — check the local '/content/angle-models-weights' folder.

**Download the weights of sam2**

``` python
import re
import os
import subprocess

def download_file_from_gdrive(link_or_id, dest_folder="/content/sam2/checkpoints", filename=None):
    m = re.search(r'id=([A-Za-z0-9_-]+)', link_or_id) or re.search(r'/d/([A-Za-z0-9_-]+)', link_or_id)
    if not m:
        raise ValueError("Paste a valid Google Drive file link")
    fid = m.group(1)

    if not os.path.exists(dest_folder):
        os.makedirs(dest_folder)

    url = f"https://drive.google.com/uc?id={fid}"

    if not filename:

        filename = f"{fid}.pt"

    file_path = os.path.join(dest_folder, filename)

    print(f"Downloading file to {file_path} ...")
    subprocess.run(["gdown", url, "-O", file_path], check=True)
    print("Download complete.")


link = "https://drive.google.com/file/d/11L0iHZ1Ktcjhd0vqKI_3-PJ3DTv2UyOc/view?usp=sharing"

download_file_from_gdrive(link, dest_folder="/content/sam2/checkpoints", filename="sam2_hiera_base_plus.pt")
```

    Downloading file to /content/sam2/checkpoints/sam2_hiera_base_plus.pt ...
    Download complete.

# **RF-DETR Model for shape and arrow detection**

``` python
dataset='/content/Dataset/AiBoardScannerDataSet.coco'
```

Metric evaluation and ploting

``` python
from PIL import Image

Image.open("/content/weights/metrics_plot.png")
```

![](00_core.backup_files/figure-commonmark/cell-8-output-1.png)

path of the pre-train weights and path of the image to to check the
shape detection

``` python
preTrainedWeights="/content/weights/checkpoint_best_regular.pth"
```

``` python
import fitz
from PIL import Image
import io
import os

file_path = "/content/Ai-diagrams-board-25-_jpg.rf.47ff3ffa292e044b3822c0c83022ee77.jpg"
output_image_path = "/content/converted_image.png"

imgPath = None

if not os.path.exists(file_path):
    print(f"Error: File not found at {file_path}")
else:
    if file_path.lower().endswith('.pdf'):
        try:
            doc = fitz.open(file_path)
            page = doc.load_page(0)
            pix = page.get_pixmap()
            pix.save(output_image_path)
            imgPath = Image.open(output_image_path).convert("RGB")
            print("PDF converted to image and saved successfully.")
            doc.close()
        except Exception as e:
            print(f"Error converting PDF: {e}")
    else:
        try:
            imgPath = Image.open(file_path).convert("RGB")
            print("Image loaded successfully.")
        except Exception as e:
            print(f"Error opening image: {e}")
```

    Image loaded successfully.

``` python
import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRMedium
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRMedium(pretrain_weights=preTrainedWeights)
model.optimize_for_inference()

image = imgPath

detections = model.predict(image, threshold=0.60)

text_scale = sv.calculate_optimal_text_scale(image.size)
thickness = sv.calculate_optimal_line_thickness(image.size)
color = sv.ColorPalette.from_hex([
    "#FFFF00", "#FF9B00", "#FF66FF", "#3399FF", "#FF66B2", "#FF8080",
    "#B266FF", "#9999FF", "#66FFFF", "#33FF99", "#66FF66", "#99FF00"
])

bbox_annotator = sv.BoxAnnotator(color=color, thickness=thickness)
label_annotator = sv.LabelAnnotator(color=color, text_color=sv.Color.BLACK, text_scale=text_scale)
my_classes = [
    "objects",
    "Cloud",
    "Diamond",
    "Double Arrow",
    "Pentagon",
    "Racetrack",
    "Star",
    "Sticky Notes",
    "Triangle",
    "arrow",
    "arrow_head",
    "circle",
    "dashed-arrow",
    "dotted-arrow",
    "rectangle",
    "rounded rectangle",
    "solid-arrow"
]
detection_labels = [
    f"{my_classes[class_id]} {conf:.2f}"
    for class_id, conf in zip(detections.class_id, detections.confidence)
]

detections_list=[]
for bbox, class_id, conf in zip(detections.xyxy, detections.class_id, detections.confidence):
    x_min, y_min, x_max, y_max = bbox
    class_name = my_classes[class_id]
    detection_tuple = (class_name, (float(x_min), float(y_min)), (float(x_max), float(y_max)))
    detections_list.append(detection_tuple)
print("detections = [")
for d in detections_list:
    print(f"    {d},")
print("]")

annotated_image = image.copy()
annotated_image = bbox_annotator.annotate(annotated_image, detections)
annotated_image = label_annotator.annotate(annotated_image, detections, detection_labels)

sv.plot_image(annotated_image)
```

    UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)

    Using a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.
    Using patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.
    Loading pretrain weights

    WARNING:rfdetr.main:num_classes mismatch: pretrain weights has 16 classes, but your model has 90 classes
    reinitializing detection head with 16 classes
    `loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
    TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
    TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
    TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
    UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
    TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).

    detections = [
        ('Racetrack', (525.43798828125, 337.20953369140625), (772.7925415039062, 731.5386962890625)),
        ('circle', (75.26021575927734, 353.7659912109375), (211.26466369628906, 485.6578369140625)),
        ('Pentagon', (197.82093811035156, 192.63380432128906), (629.0830688476562, 607.8374633789062)),
        ('Racetrack', (399.3763732910156, 37.14316940307617), (734.2215576171875, 357.04290771484375)),
        ('circle', (191.36500549316406, 624.5045776367188), (327.5169677734375, 757.7999877929688)),
        ('circle', (540.4708862304688, 566.7578735351562), (676.06787109375, 699.739501953125)),
        ('Racetrack', (91.32276916503906, 34.96403884887695), (442.41143798828125, 351.8035888671875)),
        ('circle', (579.9710083007812, 197.84326171875), (715.2356567382812, 331.45751953125)),
        ('circle', (289.7023010253906, 63.5079460144043), (423.3017883300781, 196.29493713378906)),
        ('Racetrack', (181.86737060546875, 609.5239868164062), (572.858642578125, 773.0011596679688)),
        ('Racetrack', (57.61482620239258, 337.6130676269531), (306.7226257324219, 737.0006103515625)),
    ]

![](00_core.backup_files/figure-commonmark/cell-11-output-5.png)

**Convert Co-ordinates of detected objects into JSON**

``` python
detected_objects = []
for class_id, conf, bbox in zip(detections.class_id, detections.confidence, detections.xyxy):
    x1, y1, x2, y2 = bbox.tolist()
    detected_objects.append({
        "class": my_classes[class_id],
        "confidence": float(conf),
        "bbox": {
            "x1": float(x1),
            "y1": float(y1),
            "x2": float(x2),
            "y2": float(y2)
        }
    })

import json
print(json.dumps(detected_objects, indent=2))
```

    [
      {
        "class": "Diamond",
        "confidence": 0.9714662432670593,
        "bbox": {
          "x1": 29.127370834350586,
          "y1": 407.62640380859375,
          "x2": 239.15098571777344,
          "y2": 544.3034057617188
        }
      },
      {
        "class": "Diamond",
        "confidence": 0.9698772430419922,
        "bbox": {
          "x1": 28.874351501464844,
          "y1": 227.95535278320312,
          "x2": 238.50436401367188,
          "y2": 363.88623046875
        }
      },
      {
        "class": "Racetrack",
        "confidence": 0.9692795872688293,
        "bbox": {
          "x1": 384.4695739746094,
          "y1": 255.876953125,
          "x2": 697.0001220703125,
          "y2": 361.5684509277344
        }
      },
      {
        "class": "Diamond",
        "confidence": 0.9654581546783447,
        "bbox": {
          "x1": 29.296884536743164,
          "y1": 47.461517333984375,
          "x2": 238.36622619628906,
          "y2": 183.186767578125
        }
      },
      {
        "class": "Triangle",
        "confidence": 0.9631335139274597,
        "bbox": {
          "x1": 844.4630737304688,
          "y1": 72.72488403320312,
          "x2": 1027.826416015625,
          "y2": 192.2169952392578
        }
      },
      {
        "class": "Triangle",
        "confidence": 0.96173495054245,
        "bbox": {
          "x1": 844.9676513671875,
          "y1": 233.00119018554688,
          "x2": 1028.5804443359375,
          "y2": 353.65087890625
        }
      },
      {
        "class": "solid-arrow",
        "confidence": 0.9547531008720398,
        "bbox": {
          "x1": 694.0907592773438,
          "y1": 131.23728942871094,
          "x2": 886.1165771484375,
          "y2": 313.0189208984375
        }
      },
      {
        "class": "solid-arrow",
        "confidence": 0.9540013074874878,
        "bbox": {
          "x1": 698.8170166015625,
          "y1": 304.6488037109375,
          "x2": 886.5786743164062,
          "y2": 482.6002197265625
        }
      },
      {
        "class": "Triangle",
        "confidence": 0.9502191543579102,
        "bbox": {
          "x1": 843.9762573242188,
          "y1": 394.875732421875,
          "x2": 1028.1793212890625,
          "y2": 514.8218383789062
        }
      },
      {
        "class": "solid-arrow",
        "confidence": 0.9406890869140625,
        "bbox": {
          "x1": 242.09933471679688,
          "y1": 303.71929931640625,
          "x2": 382.69677734375,
          "y2": 490.77899169921875
        }
      },
      {
        "class": "solid-arrow",
        "confidence": 0.9405144453048706,
        "bbox": {
          "x1": 242.72161865234375,
          "y1": 101.90926361083984,
          "x2": 385.8294372558594,
          "y2": 311.9975280761719
        }
      },
      {
        "class": "solid-arrow",
        "confidence": 0.8978973627090454,
        "bbox": {
          "x1": 697.5059814453125,
          "y1": 293.218994140625,
          "x2": 883.3981323242188,
          "y2": 330.16082763671875
        }
      },
      {
        "class": "solid-arrow",
        "confidence": 0.8837128281593323,
        "bbox": {
          "x1": 243.73397827148438,
          "y1": 281.9586181640625,
          "x2": 383.1846618652344,
          "y2": 313.9234619140625
        }
      },
      {
        "class": "arrow_head",
        "confidence": 0.8148650527000427,
        "bbox": {
          "x1": 243.56497192382812,
          "y1": 281.25982666015625,
          "x2": 270.8069152832031,
          "y2": 311.5813293457031
        }
      },
      {
        "class": "arrow_head",
        "confidence": 0.8067644238471985,
        "bbox": {
          "x1": 243.93441772460938,
          "y1": 462.5649719238281,
          "x2": 272.4416809082031,
          "y2": 490.833251953125
        }
      },
      {
        "class": "arrow_head",
        "confidence": 0.795447051525116,
        "bbox": {
          "x1": 862.6205444335938,
          "y1": 292.51361083984375,
          "x2": 885.913818359375,
          "y2": 316.01947021484375
        }
      },
      {
        "class": "arrow_head",
        "confidence": 0.7893730401992798,
        "bbox": {
          "x1": 243.89553833007812,
          "y1": 101.54055786132812,
          "x2": 271.93878173828125,
          "y2": 131.6016845703125
        }
      },
      {
        "class": "arrow_head",
        "confidence": 0.782149612903595,
        "bbox": {
          "x1": 866.0701293945312,
          "y1": 454.7251281738281,
          "x2": 886.1697387695312,
          "y2": 480.5311584472656
        }
      },
      {
        "class": "arrow_head",
        "confidence": 0.7689226865768433,
        "bbox": {
          "x1": 863.8754272460938,
          "y1": 130.90518188476562,
          "x2": 886.8487548828125,
          "y2": 155.73548889160156
        }
      }
    ]

# **Paddle OCR**

**saved the output in the OCRoutput folder after applying the OCR:** 1.
first image saved by after applying the ocr 2. second saved the response
in the JSON format of the OCR applied image.

``` python
pip install langchain==0.0.350
```

    Collecting langchain==0.0.350
      Downloading langchain-0.0.350-py3-none-any.whl.metadata (13 kB)
    Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (6.0.2)
    Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.0.44)
    Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (3.13.2)
    Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.350)
      Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)
    Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (1.33)
    Collecting langchain-community<0.1,>=0.0.2 (from langchain==0.0.350)
      Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)
    Collecting langchain-core<0.2,>=0.1 (from langchain==0.0.350)
      Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)
    Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.350)
      Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)
    Collecting numpy<2,>=1 (from langchain==0.0.350)
      Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 6.9 MB/s eta 0:00:00
    Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.12.3)
    Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.32.4)
    Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.350)
      Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)
    Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (2.6.1)
    Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.4.0)
    Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (25.4.0)
    Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.8.0)
    Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (6.7.0)
    Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (0.4.1)
    Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.22.0)
    Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.350)
      Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)
    Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.350)
      Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)
    Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.350) (3.0.0)
    INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.
    Collecting langchain-community<0.1,>=0.0.2 (from langchain==0.0.350)
      Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)
      Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)
      Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)
      Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)
      Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)
      Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)
      Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)
    INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.
      Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)
      Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)
      Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)
      Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)
      Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)
    INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
      Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)
      Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)
      Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)
      Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)
      Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)
      Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)
    INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.
    Collecting langchain-core<0.2,>=0.1 (from langchain==0.0.350)
      Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)
    INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.
      Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)
    INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
      Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)
      Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)
    Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1->langchain==0.0.350) (4.12.0)
      Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)
      Downloading langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)
    Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.350)
      Downloading langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)
    Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1->langchain==0.0.350)
      Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
    Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (0.7.0)
    Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (2.41.4)
    Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (4.15.0)
    Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (0.4.2)
    Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.350) (3.4.4)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.350) (3.7)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.350) (2.5.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.350) (2025.11.12)
    Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.350) (3.3.0)
    Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.350)
      Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)
    Downloading langchain-0.0.350-py3-none-any.whl (809 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 809.1/809.1 kB 48.7 MB/s eta 0:00:00
    Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)
    Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 89.1 MB/s eta 0:00:00
    Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.2/241.2 kB 30.2 MB/s eta 0:00:00
    Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.4/55.4 kB 7.0 MB/s eta 0:00:00
    Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 100.2 MB/s eta 0:00:00
    Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)
    Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 kB 6.1 MB/s eta 0:00:00
    Downloading packaging-23.2-py3-none-any.whl (53 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 5.7 MB/s eta 0:00:00
    Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)
    Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)
    Installing collected packages: tenacity, packaging, numpy, mypy-extensions, typing-inspect, marshmallow, langsmith, dataclasses-json, langchain-core, langchain-community, langchain
      Attempting uninstall: tenacity
        Found existing installation: tenacity 9.1.2
        Uninstalling tenacity-9.1.2:
          Successfully uninstalled tenacity-9.1.2
      Attempting uninstall: packaging
        Found existing installation: packaging 25.0
        Uninstalling packaging-25.0:
          Successfully uninstalled packaging-25.0
      Attempting uninstall: numpy
        Found existing installation: numpy 2.0.2
        Uninstalling numpy-2.0.2:
          Successfully uninstalled numpy-2.0.2
      Attempting uninstall: langsmith
        Found existing installation: langsmith 0.4.55
        Uninstalling langsmith-0.4.55:
          Successfully uninstalled langsmith-0.4.55
      Attempting uninstall: langchain-core
        Found existing installation: langchain-core 1.1.1
        Uninstalling langchain-core-1.1.1:
          Successfully uninstalled langchain-core-1.1.1
      Attempting uninstall: langchain
        Found existing installation: langchain 1.1.2
        Uninstalling langchain-1.1.2:
          Successfully uninstalled langchain-1.1.2
    ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.
    jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.
    opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= "3.9", but you have numpy 1.26.4 which is incompatible.
    langgraph-checkpoint 3.0.1 requires langchain-core>=0.2.38, but you have langchain-core 0.1.23 which is incompatible.
    google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.
    pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.
    jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.
    xarray 2025.11.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.
    google-adk 1.20.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.
    langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.
    db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.
    Successfully installed dataclasses-json-0.6.7 langchain-0.0.350 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.26.1 mypy-extensions-1.1.0 numpy-1.26.4 packaging-23.2 tenacity-8.5.0 typing-inspect-0.9.0

    Unable to display output for mime type(s): application/vnd.colab-display-data+json

``` python
import cv2
import numpy as np
from paddleocr import PaddleOCR
import os


if file_path.lower().endswith('.pdf'):
    image_for_ocr_path = output_image_path
else:
    image_for_ocr_path = file_path

img = cv2.imread(image_for_ocr_path)

kernel = np.array([[0, -1, 0],
                   [-1, 3, -1],
                   [0, -1, 0]])

sharpened = cv2.filter2D(img, -1, kernel)

alpha = 0.5
sharpened = cv2.addWeighted(img, 1 - alpha, sharpened, alpha, 0)

ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=True,
    lang='en',
)

result = ocr.predict(sharpened)

out_dir = "OCRoutput"
os.makedirs(out_dir, exist_ok=True)

base_name = os.path.splitext(os.path.basename(file_path))[0]

img_output_path = os.path.join(out_dir, f"{base_name}_ocr_res_img.png")
json_output_path = os.path.join(out_dir, f"{base_name}_ocr_res.json")

for i, res in enumerate(result):
    res.print()
    res.save_to_img(img_output_path)
    res.save_to_json(json_output_path)
    break
```

    Checking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.
    UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
    Creating model: ('PP-LCNet_x1_0_textline_ori', None)
    Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in `/root/.paddlex/official_models/PP-LCNet_x1_0_textline_ori`.
    UserWarning: 
    The secret `HF_TOKEN` does not exist in your Colab secrets.
    To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
    You will be able to reuse this secret in all of your notebooks.
    Please note that authentication is recommended but still optional to access public models or datasets.

    Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]

    inference.yml:   0%|          | 0.00/735 [00:00<?, ?B/s]

    inference.json: 0.00B [00:00, ?B/s]

    .gitattributes: 0.00B [00:00, ?B/s]

    config.json: 0.00B [00:00, ?B/s]

    README.md: 0.00B [00:00, ?B/s]

    inference.pdiparams:   0%|          | 0.00/6.74M [00:00<?, ?B/s]

    Creating model: ('PP-OCRv5_server_det', None)
    Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in `/root/.paddlex/official_models/PP-OCRv5_server_det`.

    Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]

    .gitattributes: 0.00B [00:00, ?B/s]

    inference.yml:   0%|          | 0.00/903 [00:00<?, ?B/s]

    inference.json: 0.00B [00:00, ?B/s]

    README.md: 0.00B [00:00, ?B/s]

    config.json: 0.00B [00:00, ?B/s]

    inference.pdiparams:   0%|          | 0.00/87.9M [00:00<?, ?B/s]

    Creating model: ('en_PP-OCRv5_mobile_rec', None)
    Using official model (en_PP-OCRv5_mobile_rec), the model files will be automatically downloaded and saved in `/root/.paddlex/official_models/en_PP-OCRv5_mobile_rec`.

    Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]

    README.md: 0.00B [00:00, ?B/s]

    inference.yml: 0.00B [00:00, ?B/s]

    config.json: 0.00B [00:00, ?B/s]

    .gitattributes: 0.00B [00:00, ?B/s]

    inference.json: 0.00B [00:00, ?B/s]

    inference.pdiparams:   0%|          | 0.00/7.77M [00:00<?, ?B/s]

    {'res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': True}, 'dt_polys': array([[[471,  87],
            ...,
            [453, 108]],

           ...,

           [[365, 713],
            ...,
            [365, 733]]], dtype=int16), 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([0, ..., 0]), 'text_rec_score_thresh': 0.0, 'return_word_box': False, 'rec_texts': ['The void always', '#1', 'stares back.', '#2', 'We rot beautifully.', 'Fragments of', 'Fire: Short', '#5', 'Truths for', 'a', 'Especially you.', 'Heavy Mind', 'Everything', 'Hope is a', 'lends.', 'dangerous', 'illusion', '#3', 'Light', 'lies;', '#4', 'shadows', 'remember.'], 'rec_scores': array([0.99967277, ..., 0.99994493]), 'rec_polys': array([[[471,  87],
            ...,
            [453, 108]],

           ...,

           [[365, 713],
            ...,
            [365, 733]]], dtype=int16), 'rec_boxes': array([[453, ..., 225],
           ...,
           [365, ..., 733]], dtype=int16)}}
    Connecting to https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/fonts/simfang.ttf ...
    Downloading simfang.ttf ...
    [==================================================] 100.00%

Get the bbox of the text extracted by the paddleOCR from the json file

``` python
import json
import math
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import os


STANDARD_ANGLES = [0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0, 360.0]
SNAP_TOLERANCE = 10.0



def _flatten_poly(poly):
    arr = np.array(poly, dtype=float)
    if arr.ndim == 1:
        arr = arr.reshape(-1, 2)
    return arr

def _box_area(b):
    return max(0, b[2] - b[0]) * max(0, b[3] - b[1])

def _intersection_area(a, b):
    ix1, iy1 = max(a[0], b[0]), max(a[1], b[1])
    ix2, iy2 = min(a[2], b[2]), min(a[3], b[3])
    return max(0, ix2 - ix1) * max(0, iy2 - iy1)



def calculate_rotation_from_polygon(polygon):

    pts = _flatten_poly(polygon)
    n = pts.shape[0]

    if n < 2:
        return 0.0

    if n == 2:
        dx = pts[1, 0] - pts[0, 0]
        dy = pts[1, 1] - pts[0, 1]
        return (math.degrees(math.atan2(-dy, dx)) + 360) % 360

    centered = pts - pts.mean(axis=0)

    if np.allclose(centered, 0, atol=1e-6):
        return 0.0

    _, _, Vt = np.linalg.svd(centered, full_matrices=False)
    vx, vy = Vt[0]
    pca_angle = (math.degrees(math.atan2(-vy, vx)) + 360) % 360


    x = pts[:, 0]
    y = pts[:, 1]
    if len(np.unique(x)) > 1:
        a, b = np.polyfit(x, y, 1)
        lf_angle = (math.degrees(math.atan2(-a, 1)) + 360) % 360
    else:
        lf_angle = pca_angle

    pca_var = np.var(centered @ np.array([vx, vy]))
    lf_dir = np.array([1, a]) / np.linalg.norm([1, a]) if len(np.unique(x)) > 1 else np.array([1, 0])
    lf_var = np.var(centered @ lf_dir)

    best = pca_angle if pca_var >= lf_var else lf_angle
    return round(best, 2)



def snap_to_standard_angle(angle, tolerance=SNAP_TOLERANCE):

    angle = angle % 360
    best = min(STANDARD_ANGLES, key=lambda s: abs(angle - s))
    return int(best) if abs(best - angle) <= tolerance else round(angle, 1)



def get_text_rotation_for_shape(shape_bbox, ocr_data, threshold=0.25):
    sx1, sy1, sx2, sy2 = shape_bbox

    weighted_vector = np.array([0.0, 0.0])
    total_weight = 0
    matched = False

    for item in ocr_data:
        tx1, ty1, tx2, ty2 = item["bbox"]
        text_area = _box_area(item["bbox"])
        if text_area == 0:
            continue

        inter = _intersection_area(shape_bbox, item["bbox"])
        overlap = inter / text_area
        if overlap < threshold:
            continue

        matched = True
        conf = float(item.get("confidence", 1.0))

        ang = calculate_rotation_from_polygon(item.get("polygon", [(tx1, ty1), (tx2, ty2)]))

        rad = math.radians(ang)
        ux, uy = math.cos(rad), math.sin(rad)

        w = overlap * conf * max(1, text_area)
        weighted_vector += w * np.array([ux, uy])
        total_weight += w

    if not matched or total_weight == 0:
        return None

    mean_vec = weighted_vector / total_weight
    mean_angle = (math.degrees(math.atan2(mean_vec[1], mean_vec[0])) + 360) % 360
    mean_angle = round(mean_angle, 2)

    snapped = snap_to_standard_angle(mean_angle)

    return {"raw_rotation": mean_angle, "shape_rotation": snapped}



def process_paddleocr_json(json_path, min_confidence=0.4):
    processed = []

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            ocr = json.load(f)
    except:
        return []

    texts = ocr.get("rec_texts", [])
    scores = ocr.get("rec_scores", [])
    polys = ocr.get("rec_polys", [])
    orientation = ocr.get("textline_orientation_angles", [])

    N = min(len(texts), len(scores), len(polys))

    for i in range(N):
        if scores[i] < min_confidence:
            continue

        poly = _flatten_poly(polys[i])
        x1, y1 = np.min(poly, axis=0)
        x2, y2 = np.max(poly, axis=0)

        raw_rot = calculate_rotation_from_polygon(poly)
        snapped = snap_to_standard_angle(raw_rot)

        processed.append({
            "text": texts[i],
            "confidence": float(scores[i]),
            "bbox": (int(x1), int(y1), int(x2), int(y2)),
            "polygon": polys[i],
            "paddleocr_orientation": orientation[i] if i < len(orientation) else None,
            "polygon_rotation": raw_rot,
            "shape_rotation": snapped
        })

    return processed



def plot_ocr_detections(data):
    if not data:
        print("No data.")
        return

    fig, ax = plt.subplots(1, figsize=(14, 12))
    ax.set_facecolor("black")

    for item in data:
        x1, y1, x2, y2 = item["bbox"]
        raw = item["polygon_rotation"]
        snap = item["shape_rotation"]

        ax.add_patch(
            patches.Rectangle((x1, y1), x2 - x1, y2 - y1, ec="r", fc="none", lw=1)
        )
        ax.text(x1, y1 - 5, f"{item['text']} ({raw}° → {snap}°)",
                fontsize=6, color="white", backgroundcolor="red")

    xs = [d["bbox"][0] for d in data] + [d["bbox"][2] for d in data]
    ys = [d["bbox"][1] for d in data] + [d["bbox"][3] for d in data]

    ax.set_xlim(min(xs) - 10, max(xs) + 10)
    ax.set_ylim(max(ys) + 10, min(ys) - 10)
    ax.invert_yaxis()
    ax.set_aspect("equal")
    plt.axis("off")
    plt.show()



base_name = os.path.splitext(os.path.basename(file_path))[0]
json_output_path = f"OCRoutput/{base_name}_ocr_res.json"

if os.path.exists(json_output_path):
    final_processed_data = process_paddleocr_json(json_output_path)
    if final_processed_data:
        print("Processed OCR data (full 360° rotation):")
        print("-" * 70)
        for item in final_processed_data:
            print(f"{item['text'][:20]:<20} | Raw: {item['polygon_rotation']:6.1f}° → Rounded: {item['shape_rotation']}°")
        print("-" * 70)
        plot_ocr_detections(final_processed_data)
else:
    print(f"JSON not found: {json_output_path}")
```

    Processed OCR data (full 360° rotation):
    ----------------------------------------------------------------------
    The void always      | Raw:  319.0° → Rounded: 315°
    #1                   | Raw:    0.0° → Rounded: 0°
    stares back.         | Raw:  321.4° → Rounded: 315°
    #2                   | Raw:    0.0° → Rounded: 0°
    We rot beautifully.  | Raw:   39.5° → Rounded: 45°
    Fragments of         | Raw:    0.0° → Rounded: 0°
    Fire: Short          | Raw:  359.4° → Rounded: 360°
    #5                   | Raw:    1.9° → Rounded: 0°
    Truths for           | Raw:    0.0° → Rounded: 0°
    a                    | Raw:    0.0° → Rounded: 0°
    Especially you.      | Raw:  285.8° → Rounded: 285.8°
    Heavy Mind           | Raw:    0.0° → Rounded: 0°
    Everything           | Raw:  284.0° → Rounded: 284.0°
    Hope is a            | Raw:  249.2° → Rounded: 249.2°
    lends.               | Raw:  285.0° → Rounded: 285.0°
    dangerous            | Raw:  250.6° → Rounded: 250.6°
    illusion             | Raw:  248.1° → Rounded: 248.1°
    #3                   | Raw:    0.0° → Rounded: 0°
    Light                | Raw:  357.4° → Rounded: 360°
    lies;                | Raw:    0.0° → Rounded: 0°
    #4                   | Raw:    0.0° → Rounded: 0°
    shadows              | Raw:  358.9° → Rounded: 360°
    remember.            | Raw:    0.0° → Rounded: 0°
    ----------------------------------------------------------------------

![](00_core.backup_files/figure-commonmark/cell-15-output-2.png)

# **Spatial Relationship**

Main pipeline where find out the main relationship of the arrows with
the shapes and the detections of the arrow’s endpoints

``` python
import cv2
import matplotlib.pyplot as plt
from skimage.morphology import skeletonize
from collections import deque
import itertools
import math

# ==============================================================================
# ---------- HELPER FUNCTIONS  ----------
# ==============================================================================

def show_image(title, img, cmap=None):
    plt.figure(figsize=(4, 4))
    if len(img.shape) == 2: plt.imshow(img, cmap=cmap or "gray")
    else: plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title(title); plt.axis("off"); plt.show()

def crop_image_region(image, top_left, bottom_right):
    x1, y1 = map(int, map(round, top_left))
    x2, y2 = map(int, map(round, bottom_right))
    return image[y1:y2, x1:x2]

def convert_to_binary_mask(cropped_img):
    if cropped_img.size == 0:
        return np.array([], dtype=np.uint8)
    gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (3,3), 0)

    edges = cv2.Canny(blurred, threshold1=30, threshold2=100)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))
    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=1)

    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    if np.mean(thresh) > 127:
        thresh = cv2.bitwise_not(thresh)
    combined_mask = cv2.bitwise_or(closed, thresh)
    return combined_mask


def connect_dotted_simple_morph(binary_mask, kernel_size=5, iterations=2):

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
    closed_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel, iterations=1)
    connected_mask = cv2.dilate(closed_mask, kernel, iterations=iterations)
    return connected_mask


def find_farthest_points_in_contour(contour):
    max_dist_sq, best_pair = -1, None
    points = contour.squeeze(axis=1)
    if len(points) < 2: return None
    for p1, p2 in itertools.combinations(points, 2):
        dist_sq = (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2
        if dist_sq > max_dist_sq: max_dist_sq, best_pair = dist_sq, (tuple(p1), tuple(p2))
    return best_pair

def interpolate_dashed_arrows(binary_mask, max_gap_ratio=0.5, thickness=2):
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = [c for c in contours if cv2.contourArea(c) > 3]

    if len(contours) < 2:
        return binary_mask

    dash_endpoints = []
    for cnt in contours:
        end_pts = find_farthest_points_in_contour(cnt)
        if end_pts:
            dash_endpoints.append(list(end_pts))

    dash_lengths = [math.dist(ep[0], ep[1]) for ep in dash_endpoints if ep]
    if not dash_lengths:
        return binary_mask
    max_connect_dist = np.median(dash_lengths) * (1 + max_gap_ratio)

    output_mask = binary_mask.copy()

    for i, eps1 in enumerate(dash_endpoints):
        for ep1 in eps1:
            min_dist = float('inf')
            best_ep = None
            for j, eps2 in enumerate(dash_endpoints):
                if i == j:
                    continue
                for ep2 in eps2:
                    dist = math.dist(ep1, ep2)
                    if dist < min_dist:
                        min_dist = dist
                        best_ep = ep2
            if best_ep and min_dist < max_connect_dist:
                cv2.line(output_mask, ep1, best_ep, 255, thickness=thickness)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 5))
    output_mask = cv2.dilate(output_mask, kernel, iterations=2)

    return output_mask


def extract_skeleton(binary_mask):
    return skeletonize(binary_mask > 0).astype(np.uint8) * 255


def get_skeleton_graph_nodes(skeleton):
    endpoints, junctions = [], []
    h, w = skeleton.shape
    padded_skeleton = np.pad(skeleton, 1, 'constant')
    for y_pad in range(1, h + 1):
        for x_pad in range(1, w + 1):
            if padded_skeleton[y_pad, x_pad] > 0:
                num_neighbors = np.sum(padded_skeleton[y_pad-1:y_pad+2, x_pad-1:x_pad+2] > 0) - 1
                if num_neighbors == 1: endpoints.append((x_pad - 1, y_pad - 1))
                elif num_neighbors > 2: junctions.append((x_pad - 1, y_pad - 1))
    return endpoints, junctions


def find_farthest_points_euclidean(binary_mask):
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return []
    all_points = np.vstack([cnt.squeeze(axis=1) for cnt in contours if cnt.ndim > 2])
    if len(all_points) < 2: return []
    return find_farthest_points_in_contour(np.array(all_points).reshape(-1, 1, 2))


def check_bbox_intersection(box1_tl, box1_br, box2_tl, box2_br):
    return not (box1_br[0] < box2_tl[0] or box1_tl[0] > box2_br[0] or box1_br[1] < box2_tl[1] or box1_tl[1] > box2_br[1])


def simple_bridge_by_interpolation(binary_mask, ocr_bbox_local, thickness=1):

    h, w = binary_mask.shape[:2]
    x1, y1, x2, y2 = map(int, ocr_bbox_local)
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)
    if x2 <= x1 or y2 <= y1:
        return binary_mask.copy()

    mask_no_text = binary_mask.copy()
    mask_no_text[y1:y2, x1:x2] = 0

    pts = np.column_stack(np.where(mask_no_text > 0))
    if pts.size == 0:
        return binary_mask.copy()

    left_pts = pts[pts[:, 1] < x1]
    right_pts = pts[pts[:, 1] > x2]

    if left_pts.size and right_pts.size:
        left_idx = np.argmax(left_pts[:, 1])
        right_idx = np.argmin(right_pts[:, 1])
        left_p = (int(left_pts[left_idx, 1]), int(left_pts[left_idx, 0]))
        right_p = (int(right_pts[right_idx, 1]), int(right_pts[right_idx, 0]))
        p1, p2 = left_p, right_p
    else:
        top_pts = pts[pts[:, 0] < y1]
        bottom_pts = pts[pts[:, 0] > y2]
        if top_pts.size and bottom_pts.size:
            top_idx = np.argmax(top_pts[:, 0])
            bot_idx = np.argmin(bottom_pts[:, 0])
            top_p = (int(top_pts[top_idx, 1]), int(top_pts[top_idx, 0]))
            bot_p = (int(bottom_pts[bot_idx, 1]), int(bottom_pts[bot_idx, 0]))
            p1, p2 = top_p, bot_p
        else:
            coords = np.column_stack((pts[:,1], pts[:,0]))
            max_d = -1; best = None
            for i in range(len(coords)):
                for j in range(i+1, len(coords)):
                    d = (coords[i,0]-coords[j,0])**2 + (coords[i,1]-coords[j,1])**2
                    if d > max_d:
                        max_d = d
                        best = (tuple(coords[i].tolist()), tuple(coords[j].tolist()))
            if not best:
                return binary_mask.copy()
            p1, p2 = best

    out = mask_no_text.copy()
    dist = int(math.hypot(p2[0]-p1[0], p2[1]-p1[1]))
    if dist < 2:
        cv2.line(out, p1, p2, 255, thickness=max(1, thickness))
        return out

    n = max(dist, 2)
    xs = np.linspace(p1[0], p2[0], n)
    ys = np.linspace(p1[1], p2[1], n)
    for x, y in zip(xs, ys):
        cx, cy = int(round(x)), int(round(y))
        x0, x1_ = max(0, cx - thickness), min(w, cx + thickness + 1)
        y0, y1_ = max(0, cy - thickness), min(h, cy + thickness + 1)
        out[y0:y1_, x0:x1_] = 255

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    out = cv2.dilate(out, kernel, iterations=1)
    return out

def bbox_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    if interArea == 0:
        return 0.0
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    return interArea / float(boxAArea + boxBArea - interArea)

def pixel_level_ocr_interference(binary_arrow_mask, ocr_bbox, crop_origin, coverage_threshold=0.05):
    crop_x1, crop_y1 = crop_origin
    h, w = binary_arrow_mask.shape

    ox1, oy1, ox2, oy2 = ocr_bbox
    local_x1 = max(0, int(ox1 - crop_x1))
    local_y1 = max(0, int(oy1 - crop_y1))
    local_x2 = min(w, int(ox2 - crop_x1))
    local_y2 = min(h, int(oy2 - crop_y1))

    if local_x1 >= local_x2 or local_y1 >= local_y2:
        return False

    arrow_pixels = np.count_nonzero(binary_arrow_mask)
    if arrow_pixels == 0:
        return False

    overlap_region = binary_arrow_mask[local_y1:local_y2, local_x1:local_x2]
    overlap_count = np.count_nonzero(overlap_region)
    coverage_ratio = overlap_count / arrow_pixels
    return coverage_ratio > coverage_threshold

def mask_overlapping_ocr_regions(binary_mask, ocr_intrusions, crop_origin):
    mask = binary_mask.copy()
    crop_x1, crop_y1 = crop_origin
    h, w = mask.shape

    for ocr in ocr_intrusions:
        ox1, oy1, ox2, oy2 = ocr['bbox']
        local_x1 = max(0, int(ox1 - crop_x1))
        local_y1 = max(0, int(oy1 - crop_y1))
        local_x2 = min(w, int(ox2 - crop_x1))
        local_y2 = min(h, int(oy2 - crop_y1))
        if local_x1 < local_x2 and local_y1 < local_y2:
            mask[local_y1:local_y2, local_x1:local_x2] = 0
    return mask

def match_arrowhead_to_endpoint(arrowhead_bboxes, endpoints, crop_origin):

    if not arrowhead_bboxes:
        return None

    crop_x, crop_y = crop_origin
    arrowhead_centers = []
    for tl, br in arrowhead_bboxes:
        cx = (tl[0] + br[0]) / 2 - crop_x
        cy = (tl[1] + br[1]) / 2 - crop_y
        arrowhead_centers.append((cx, cy))

    for center in arrowhead_centers:
        for ep in endpoints:

            if abs(ep[0] - center[0]) < 15 and abs(ep[1] - center[1]) < 12:
                return ep
    return None

# =================================================================================
# ---------- FOR INTERSECTING ARROWS ----------
# =================================================================================

def bfs_get_path(skeleton, start_xy, end_xy):


    h, w = skeleton.shape
    start_rc, end_rc = (start_xy[1], start_xy[0]), (end_xy[1], end_xy[0])

    if not (0 <= start_rc[0] < h and 0 <= start_rc[1] < w and skeleton[start_rc] > 0): return None
    if not (0 <= end_rc[0] < h and 0 <= end_rc[1] < w and skeleton[end_rc] > 0): return None

    q = deque([(start_rc, [start_xy])])
    visited = {start_rc}

    while q:
        (r, c), path = q.popleft()

        if (r, c) == end_rc:
            return path

        for dr, dc in itertools.product([-1, 0, 1], repeat=2):
            if dr == 0 and dc == 0: continue
            nr, nc = r + dr, c + dc

            if 0 <= nr < h and 0 <= nc < w and skeleton[nr, nc] > 0 and (nr, nc) not in visited:
                visited.add((nr, nc))
                new_path = path + [(nc, nr)]
                q.append(((nr, nc), new_path))

    return None

def find_best_arrow_by_straightness(binary_mask, min_path_length=20):

    skeleton = extract_skeleton(binary_mask)
    if np.count_nonzero(skeleton) < min_path_length:
        return None

    endpoints, _ = get_skeleton_graph_nodes(skeleton)

    if len(endpoints) < 2:
        return find_farthest_points_euclidean(binary_mask)

    candidate_paths = []
    for p1, p2 in itertools.combinations(endpoints, 2):
        path = bfs_get_path(skeleton, p1, p2)

        if path and len(path) >= min_path_length:
            path_length = len(path)
            euclidean_dist = math.dist(p1, p2)

            straightness = euclidean_dist / path_length

            score = path_length * straightness

            candidate_paths.append({
                "endpoints": [p1, p2],
                "score": score,
                "length": path_length
            })

    if not candidate_paths:
        return None

    best_path = max(candidate_paths, key=lambda x: x['score'])
    return best_path['endpoints']


# ==============================================================================
# ----------   MAIN PIPELINE FUNCTION  ----------
# ==============================================================================

def skeleton_has_cycle(skeleton):

    h, w = skeleton.shape
    visited = np.zeros((h, w), dtype=bool)

    def neighbors(r, c):
        for dr in [-1, 0, 1]:
            for dc in [-1, 0, 1]:
                if dr == 0 and dc == 0:
                    continue
                nr, nc = r + dr, c + dc
                if 0 <= nr < h and 0 <= nc < w and skeleton[nr, nc] > 0:
                    yield nr, nc

    def dfs(r, c, pr, pc):
        visited[r, c] = True
        for nr, nc in neighbors(r, c):
            if not visited[nr, nc]:
                if dfs(nr, nc, r, c):
                    return True
            elif (nr, nc) != (pr, pc):
                return True
        return False

    for y in range(h):
        for x in range(w):
            if skeleton[y, x] > 0 and not visited[y, x]:
                if dfs(y, x, -1, -1):
                    return True
    return False

def find_farthest_geodesic_endpoints(skeleton):
    endpoints, _ = get_skeleton_graph_nodes(skeleton)
    if len(endpoints) < 2: return None
    max_length, best_pair = -1, None
    for p1, p2 in itertools.combinations(endpoints, 2):
        path = bfs_get_path(skeleton, p1, p2)
        if path and len(path) > max_length:
            max_length, best_pair = len(path), (p1, p2)
    return best_pair


def dilate_ocr_regions(binary_mask, ocr_intrusions, crop_origin, vert_expand=1, horiz_expand=1, vert_iterations=1, horiz_iterations=1, erosion_kernel_size=3, erosion_iterations=1):
    mask = np.zeros_like(binary_mask)
    crop_x1, crop_y1 = crop_origin
    h, w = mask.shape
    for ocr in ocr_intrusions:
        ox1, oy1, ox2, oy2 = ocr['bbox']
        local_x1 = max(0, int(ox1 - crop_x1))
        local_y1 = max(0, int(oy1 - crop_y1))
        local_x2 = min(w, int(ox2 - crop_x1))
        local_y2 = min(h, int(oy2 - crop_y1))
        if local_x1 < local_x2 and local_y1 < local_y2:
            mask[local_y1:local_y2, local_x1:local_x2] = 255

    vert_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vert_expand))
    horiz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (horiz_expand, 1))

    mask = cv2.dilate(mask, vert_kernel, iterations=vert_iterations)
    mask = cv2.dilate(mask, horiz_kernel, iterations=horiz_iterations)

    combined = cv2.bitwise_or(binary_mask, mask)

    erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (erosion_kernel_size, erosion_kernel_size))
    combined = cv2.erode(combined, erosion_kernel, iterations=erosion_iterations)

    return combined

def find_skeleton_points_on_bbox(skeleton, bbox_tl, bbox_br, margin=2):

    points = np.column_stack(np.where(skeleton > 0))
    x_min, y_min = bbox_tl
    x_max, y_max = bbox_br
    candidate_points = []
    for y, x in points:
        near_left = abs(x - x_min) <= margin and y_min <= y <= y_max
        near_right = abs(x - x_max) <= margin and y_min <= y <= y_max
        near_top = abs(y - y_min) <= margin and x_min <= x <= x_max
        near_bottom = abs(y - y_max) <= margin and x_min <= x <= x_max
        if near_left or near_right or near_top or near_bottom:
            candidate_points.append((x, y))
    return candidate_points



def debug_stage(image, detections, ocr_results):
    vectors = []
    arrow_labels = ["dashed-arrow", "dotted-arrow", "solid-arrow"]

    non_arrow_labels = ["Diamond"]

    arrows = [d for d in detections if d[0] in arrow_labels]
    interfering_objects = [d for d in detections if d[0] in non_arrow_labels]

    for idx, (label, top_left, bottom_right) in enumerate(arrows):
        print(f"\n=== Processing Arrow {idx} ({label}) ===")
        crop_x1, crop_y1 = int(top_left[0]), int(top_left[1])
        crop = crop_image_region(image, top_left, bottom_right)
        if crop.size == 0:
            continue
        show_image(f"Crop {idx}", crop)

        binary_mask = convert_to_binary_mask(crop)
        if binary_mask.size == 0:
            continue

        processed_mask = binary_mask.copy()
        endpoints = None

        is_arrow_arrow_overlap = any(
            idx != other_idx and check_bbox_intersection(top_left, bottom_right, other_tl, other_br)
            for other_idx, (_, other_tl, other_br) in enumerate(arrows)
        )

        ocr_intrusions = []
        if ocr_results:
            for ocr in ocr_results:
                if bbox_iou((top_left[0], top_left[1], bottom_right[0], bottom_right[1]), ocr['bbox']) > 0.02:
                    if pixel_level_ocr_interference(processed_mask, ocr['bbox'], (crop_x1, crop_y1), coverage_threshold=0.02):
                        ocr_intrusions.append(ocr)

        ocr_interrupted = len(ocr_intrusions) > 0

        if not ocr_interrupted:
            if label == "dashed-arrow":
                processed_mask = interpolate_dashed_arrows(processed_mask)
            elif label == "dotted-arrow":
                processed_mask = connect_dotted_simple_morph(processed_mask, kernel_size=5, iterations=2)

        if ocr_interrupted:
            print("--> Strategy: OCR interruption detected. Masking OCR and applying interpolation on all.")
            processed_mask = mask_overlapping_ocr_regions(processed_mask, ocr_intrusions, (crop_x1, crop_y1))
            for ocr in ocr_intrusions:
                ox1, oy1, ox2, oy2 = ocr['bbox']
                o_local = (ox1 - crop_x1, oy1 - crop_y1, ox2 - crop_x1, oy2 - crop_y1)

            processed_mask = dilate_ocr_regions(
                processed_mask, ocr_intrusions, (crop_x1, crop_y1),
                vert_expand=5, horiz_expand=10, vert_iterations=3, horiz_iterations=1
            )
            processed_mask = connect_dotted_simple_morph(processed_mask, kernel_size=6)

            skeleton = extract_skeleton(processed_mask)
            if skeleton_has_cycle(skeleton):
                print("Cycle detected in skeleton within OCR interrupted region")

                intersecting_shapes = [d for d in interfering_objects if check_bbox_intersection(top_left, bottom_right, d[1], d[2])]
                all_intersection_points = []
                for shape_tl, shape_br in [d[1:3] for d in intersecting_shapes]:
                    local_shape_tl = (shape_tl[0] - crop_x1, shape_tl[1] - crop_y1)
                    local_shape_br = (shape_br[0] - crop_x1, shape_br[1] - crop_y1)
                    points_on_bbox = find_skeleton_points_on_bbox(
                        skeleton, local_shape_tl, local_shape_br, margin=2
                    )
                    all_intersection_points.extend(points_on_bbox)

                if len(all_intersection_points) >= 2:
                    max_dist = -1
                    endpoints = None
                    for p1, p2 in itertools.combinations(all_intersection_points, 2):
                        dist = math.dist(p1, p2)
                        if dist > max_dist:
                            max_dist = dist
                            endpoints = [p1, p2]
                    endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]
                else:
                    endpoints = find_best_arrow_by_straightness(processed_mask)
                    if endpoints and len(endpoints) == 2:
                        endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]
            else:
                endpoints = find_best_arrow_by_straightness(processed_mask)
                if not endpoints or len(endpoints) < 2:
                    endpoints = find_farthest_points_euclidean(processed_mask)
                if endpoints:
                    endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]

        elif is_arrow_arrow_overlap:
            print("--> Strategy: Overlap detected. Using straightness scoring.")
            endpoints = find_best_arrow_by_straightness(processed_mask)
            if not endpoints:
                print("--> Scoring failed. Falling back to farthest points.")
                endpoints = find_farthest_points_euclidean(processed_mask)
            if endpoints:
                endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]
        else:
            print("--> Strategy: General arrow analysis with straightness scoring.")
            endpoints = find_best_arrow_by_straightness(processed_mask)
            if endpoints:
                endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]

        show_image(f"Final Processed Mask {idx}", processed_mask, cmap="gray")

        if not endpoints or len(endpoints) < 2:
            print(f"-> Arrow {idx}: FAILED to find 2 endpoints.")
            continue

        arrowhead_detections = [d for d in detections if d[0] == "arrow_head" and check_bbox_intersection(top_left, bottom_right, d[1], d[2])]
        arrowhead_bboxes = [(d[1], d[2]) for d in arrowhead_detections]

        matched_tail = match_arrowhead_to_endpoint(arrowhead_bboxes, endpoints, (crop_x1, crop_y1))

        if matched_tail is not None:
            tail_global = matched_tail
            head_global = endpoints[1] if endpoints[0] == tail_global else endpoints[0]
        else:
            width = bottom_right[0] - top_left[0]
            height = bottom_right[1] - top_left[1]
            if width >= height:
                tail_global, head_global = sorted(endpoints, key=lambda e: e[0])
            else:
                tail_global, head_global = sorted(endpoints, key=lambda e: e[1])

        tail_local = (tail_global[0] - crop_x1, tail_global[1] - crop_y1)
        head_local = (head_global[0] - crop_x1, head_global[1] - crop_y1)

        vis_endpoints = crop.copy()
        cv2.circle(vis_endpoints, (int(head_local[0]), int(head_local[1])), 5, (0, 0, 255), -1)
        cv2.circle(vis_endpoints, (int(tail_local[0]), int(tail_local[1])), 5, (0, 255, 0), -1)
        show_image(f"Endpoints {idx}", vis_endpoints)

        vectors.append({
            "tail": tail_global, "head": head_global,
            "bbox": (top_left, bottom_right), "label": label
        })

    return vectors



image = cv2.imread(file_path)
detections = detections_list
vectors = debug_stage(image, detections,final_processed_data)
print("\n=== Results from Full Pipeline ===")
for v in vectors:
  print(f"Arrow: Tail={v['tail']} → Head={v['head']} -> Label={v['label']} ")
```


    === Results from Full Pipeline ===

``` python
from scipy.interpolate import splprep, splev

def show_image(title, img, cmap=None):
    plt.figure(figsize=(12, 12))
    if len(img.shape) == 2:
        plt.imshow(img, cmap=cmap or "gray")
    else:
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title(title, fontsize=16)
    plt.axis("off")
    plt.show()

def point_to_bbox_distance(point, top_left, bottom_right):
    px, py = point
    x1, y1 = top_left
    x2, y2 = bottom_right

    if x1 <= px <= x2 and y1 <= py <= y2:
        return 0.0

    dx = max(x1 - px, 0, px - x2)
    dy = max(y1 - py, 0, py - y2)
    return math.sqrt(dx*dx + dy*dy)

def nearest_point_on_bbox(point, top_left, bottom_right):
    px, py = point
    x1, y1 = top_left
    x2, y2 = bottom_right
    cx = min(max(px, x1), x2)
    cy = min(max(py, y1), y2)
    return (int(round(cx)), int(round(cy)))

def find_nearest_shape_bbox(point, shapes, max_distance=None):

    if not shapes:
        return None, None, None

    min_dist = float('inf')
    nearest_shape = None
    nearest_pt = None

    for shape in shapes:
        label, top_left, bottom_right = shape
        pt_on_bbox = nearest_point_on_bbox(point, top_left, bottom_right)
        dist = math.hypot(pt_on_bbox[0] - point[0], pt_on_bbox[1] - point[1])

        if dist < min_dist:
            min_dist = dist
            nearest_shape = shape
            nearest_pt = pt_on_bbox

    if max_distance is not None and min_dist > max_distance:
        return None, None, None

    return nearest_shape, nearest_pt, min_dist


def draw_connections_on_image(image, connections):
    vis_image = image.copy()

    ARROW_COLOR = (0, 255, 0)       # green
    HEAD_COLOR = (255, 0, 0)        # red
    TAIL_COLOR = (0, 255, 255)      # cyan
    CONNECTION_COLOR = (255, 0, 255) # magenta

    for conn in connections:
        head = conn["head"]
        tail = conn["tail"]


        cv2.arrowedLine(vis_image, tail, head, ARROW_COLOR, 2, tipLength=0.04)


        cv2.circle(vis_image, head, 8, HEAD_COLOR, -1)
        cv2.circle(vis_image, tail, 8, TAIL_COLOR, -1)

        if conn.get("head_connected_to") is not None:
            head_pt = conn.get("head_connection_point")
            if head_pt is None:

                _, tl, br = conn["head_connected_to"]
                head_pt = (int((tl[0] + br[0]) / 2), int((tl[1] + br[1]) / 2))
            cv2.line(vis_image, head, head_pt, CONNECTION_COLOR, 2, cv2.LINE_AA)

            cv2.circle(vis_image, head_pt, 5, CONNECTION_COLOR, -1)

        if conn.get("tail_connected_to") is not None:
            tail_pt = conn.get("tail_connection_point")
            if tail_pt is None:
                _, tl, br = conn["tail_connected_to"]
                tail_pt = (int((tl[0] + br[0]) / 2), int((tl[1] + br[1]) / 2))
            cv2.line(vis_image, tail, tail_pt, CONNECTION_COLOR, 2, cv2.LINE_AA)
            cv2.circle(vis_image, tail_pt, 5, CONNECTION_COLOR, -1)

    return vis_image



def establish_connections(vectors, all_detections):
    labels_to_exclude = ["dashed-arrow", "dotted-arrow", "solid-arrow", "arrow_head"]
    shape_detections = [d for d in all_detections if d[0] not in labels_to_exclude]

    connections = []

    for vector in vectors:
        head_point = vector["head"]
        tail_point = vector["tail"]

        MAX_CONNECTION_DISTANCE = 44

        head_shape, head_pt_on_bbox, head_dist = find_nearest_shape_bbox(
            head_point, shape_detections, max_distance=MAX_CONNECTION_DISTANCE
        )
        tail_shape, tail_pt_on_bbox, tail_dist = find_nearest_shape_bbox(
            tail_point, shape_detections, max_distance=MAX_CONNECTION_DISTANCE
        )

        connections.append({
            "head": head_point,
            "tail": tail_point,
            "head_connected_to": head_shape,
            "head_connection_point": head_pt_on_bbox,
            "head_connection_dist": head_dist,
            "tail_connected_to": tail_shape,
            "tail_connection_point": tail_pt_on_bbox,
            "tail_connection_dist": tail_dist,
            "original_label": vector.get("label", "")
        })

    return connections



arrow_connections = establish_connections(vectors, detections)


print("\n--- ARROW CONNECTION RESULTS ---")
for i, conn in enumerate(arrow_connections):
    head_label = conn['head_connected_to'][0] if conn['head_connected_to'] else 'None'
    tail_label = conn['tail_connected_to'][0] if conn['tail_connected_to'] else 'None'
    print(
        f"Arrow {i} ({conn['original_label']}):\n"
        f"  - TAIL at {conn['tail']} connects to -> {tail_label}\n"
        f"  - HEAD at {conn['head']} connects to -> {head_label}\n"
    )

visualization_image = draw_connections_on_image(image, arrow_connections)

show_image("Final Arrow Connections", visualization_image)
```


    --- ARROW CONNECTION RESULTS ---

![](00_core.backup_files/figure-commonmark/cell-17-output-2.png)

# **Angle Prediction of shapes**

**Run once when the notebook is run from start**

``` python
```

    /content/sam2

``` python
def get_text_rotation_for_shape(shape_bbox, ocr_data, threshold=0.5):

    sx1, sy1, sx2, sy2 = shape_bbox
    shape_area = (sx2 - sx1) * (sy2 - sy1)

    best_overlap = 0
    best_rotation = None

    for ocr_item in ocr_data:
        tx1, ty1, tx2, ty2 = ocr_item['bbox']

        ix1 = max(sx1, tx1)
        iy1 = max(sy1, ty1)
        ix2 = min(sx2, tx2)
        iy2 = min(sy2, ty2)

        if ix1 < ix2 and iy1 < iy2:
            intersection = (ix2 - ix1) * (iy2 - iy1)
            text_area = (tx2 - tx1) * (ty2 - ty1)

            overlap_ratio = intersection / text_area if text_area > 0 else 0

            if overlap_ratio > threshold and overlap_ratio > best_overlap:
                best_overlap = overlap_ratio
                best_rotation = ocr_item.get('polygon_rotation',
                                            ocr_item.get('paddleocr_orientation', 0))

    return best_rotation
```

``` python
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import os
import matplotlib.pyplot as plt
import numpy as np
import cv2
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
def check_overlap(box1, box2):
    x1_a, y1_a, x2_a, y2_a = box1
    x1_b, y1_b, x2_b, y2_b = box2
    if x2_a < x1_b or x2_b < x1_a:
        return False
    if y2_a < y1_b or y2_b < y1_a:
        return False
    return True
detections = detections_list
include_labels = set([
    "Pentagon", "Racetrack",
    "Triangle", "arrow", "rectangle"
])
filtered_detections = [det for det in detections if det[0] in include_labels]
boxes = [[float(x1), float(y1), float(x2), float(y2)] for _, (x1, y1), (x2, y2) in filtered_detections]
os.chdir("/content/sam2/sam2/configs/sam2")
model_cfg = "sam2_hiera_b+.yaml"
sam2_checkpoint = "/content/sam2/checkpoints/sam2_hiera_base_plus.pt"
sam2_model = build_sam2(model_cfg, sam2_checkpoint, device="cpu")
predictor = SAM2ImagePredictor(sam2_model)
image = Image.open(file_path).convert("RGB")
completable_shapes = {'circle', 'rounded rectangle', 'rectangle', 'Racetrack'}
occlusion_flags = []
for i in range(len(filtered_detections)):
    is_occluded = False
    label_i = filtered_detections[i][0]
    box_i = boxes[i]
    if label_i in completable_shapes:
        for j in range(len(filtered_detections)):
            if i == j: continue
            box_j = boxes[j]
            if check_overlap(box_i, box_j):
                is_occluded = True
                break
    occlusion_flags.append(is_occluded)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
angle_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])
])
model_configs = {
    'rectangle': {"path": "/content/angle-models-weights/final_rectangle.pth", "num_classes": 4},
    'Triangle': {"path": "/content/angle-models-weights/Triangle.pth", "num_classes": 4},
    'Racetrack': {"path": "/content/angle-models-weights/final_racetrack.pth", "num_classes": 4},
    'Pentagon': {"path": "/content/angle-models-weights/best_resnet18_pentagon.pth", "num_classes": 2},
    'arrow': {"path": "/content/angle-models-weights/best_resnet18_arrow.pth", "num_classes": 4},
}
angle_models = {}
for shape_name, config in model_configs.items():
    if os.path.exists(config["path"]):
        model = models.resnet18(pretrained=False)
        model.fc = nn.Linear(model.fc.in_features, config["num_classes"])
        model.load_state_dict(torch.load(config["path"], map_location=device))
        model = model.to(device)
        model.eval()
        angle_models[shape_name] = model
        print(f"Loaded angle model for {shape_name} from {config['path']}")
    else:
        print(f"Warning: Angle model for {shape_name} not found at {config['path']}")
triangle_aux_model_path = "/content/angle-models-weights/Triangle_Angle_90_270.pth"
if os.path.exists(triangle_aux_model_path):
    triangle_aux_model = models.resnet18(pretrained=False)
    triangle_aux_model.fc = nn.Linear(triangle_aux_model.fc.in_features, 2)
    triangle_aux_model.load_state_dict(torch.load(triangle_aux_model_path, map_location=device))
    triangle_aux_model = triangle_aux_model.to(device)
    triangle_aux_model.eval()
    angle_models["triangle_aux"] = triangle_aux_model
def predict_angle(pil_image, model):
    img_tensor = angle_transform(pil_image).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(img_tensor)
        _, pred = torch.max(outputs, 1)
    return pred.item()
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.set_image(image)
    masks, iou_predictions, _ = predictor.predict(box=boxes)
print(f"Processed {len(boxes)} bounding boxes.")
print(f"Received {len(masks)} sets of mask candidates.")

predicted_angles = []
for i in range(len(boxes)):
    label = filtered_detections[i][0]
    is_occluded = occlusion_flags[i]
    original_bbox = boxes[i]
    box_masks = masks[i]
    box_scores = iou_predictions[i]
    best_mask_index = torch.argmax(torch.from_numpy(box_scores))
    best_mask = box_masks[best_mask_index]
    mask_binary = (best_mask > 0).astype(np.uint8)
    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        largest_contour = max(contours, key=cv2.contourArea)
        processed_mask = np.zeros_like(mask_binary)
        if label in completable_shapes and is_occluded:
            print(f"Applying occlusion correction to '{label}' (Box {i}).")
            if label == 'circle':
                (x, y), radius = cv2.minEnclosingCircle(largest_contour)
                cv2.circle(processed_mask, (int(x), int(y)), int(radius), (1), thickness=cv2.FILLED)
            elif label == 'Racetrack':
                rect = cv2.minAreaRect(largest_contour)
                center, (width, height), angle = rect
                if width < height:
                    width, height = height, width
                    angle += 90
                radius = height / 2
                if width <= height:
                    cv2.circle(processed_mask, (int(center[0]), int(center[1])), int(radius), (1), thickness=cv2.FILLED)
                else:
                    rect_center = np.array(center)
                    angle_rad = np.deg2rad(angle)
                    half_rect_len = width / 2 - radius
                    dx = np.cos(angle_rad) * half_rect_len
                    dy = np.sin(angle_rad) * half_rect_len
                    p1 = rect_center + np.array([dx, dy])
                    p2 = rect_center - np.array([dx, dy])
                    cv2.circle(processed_mask, (int(p1[0]), int(p1[1])), int(radius), (1), thickness=cv2.FILLED)
                    cv2.circle(processed_mask, (int(p2[0]), int(p2[1])), int(radius), (1), thickness=cv2.FILLED)
                    perp_dx = -np.sin(angle_rad) * radius
                    perp_dy = np.cos(angle_rad) * radius
                    corner1 = p1 + np.array([perp_dx, perp_dy])
                    corner2 = p1 - np.array([perp_dx, perp_dy])
                    corner3 = p2 - np.array([perp_dx, perp_dy])
                    corner4 = p2 + np.array([perp_dx, perp_dy])
                    rect_contour = np.array([corner1, corner2, corner3, corner4], dtype=np.intp)
                    cv2.drawContours(processed_mask, [rect_contour], 0, (1), thickness=cv2.FILLED)
            else:
                hull = cv2.convexHull(largest_contour)
                cv2.drawContours(processed_mask, [hull], -1, (1), thickness=cv2.FILLED)
        else:
            cv2.drawContours(processed_mask, [largest_contour], -1, (1), thickness=cv2.FILLED)
        mask_binary = processed_mask

    rows = np.any(mask_binary, axis=1)
    cols = np.any(mask_binary, axis=0)
    if not rows.any() or not cols.any():
        print(f"Mask for box {i} is empty after processing, skipping.")
        continue
    rmin, rmax = np.where(rows)[0][[0, -1]]
    cmin, cmax = np.where(cols)[0][[0, -1]]
    padding_percentage = 0.1
    height_pad = rmax - rmin
    width_pad = cmax - cmin
    padding_y = int(height_pad * padding_percentage)
    padding_x = int(width_pad * padding_percentage)
    padded_rmin = rmin - padding_y
    padded_rmax = rmax + padding_y
    padded_cmin = cmin - padding_x
    padded_cmax = cmax + padding_x
    full_mask_height, full_mask_width = mask_binary.shape
    padded_rmin = max(0, padded_rmin)
    padded_rmax = min(full_mask_height - 1, padded_rmax)
    padded_cmin = max(0, padded_cmin)
    padded_cmax = min(full_mask_width - 1, padded_cmax)
    padded_crop = mask_binary[padded_rmin:padded_rmax+1, padded_cmin:padded_cmax+1]
    final_image_rgb = np.zeros((padded_crop.shape[0], padded_crop.shape[1], 3), dtype=np.uint8)
    random_color = np.random.randint(0, 256, size=3)
    for c in range(3):
        final_image_rgb[:, :, c][padded_crop == 1] = random_color[c]
    cropped_pil_image = Image.fromarray(final_image_rgb)

    predicted_angle_info = "N/A"
    angle_source = "none"


    x1, y1, x2, y2 = original_bbox
    shape_bbox = (int(x1), int(y1), int(x2), int(y2))

    if 'final_processed_data' in dir() and final_processed_data:
        text_rotation = get_text_rotation_for_shape(shape_bbox, final_processed_data)
        if text_rotation is not None:
            normalized_rotation = text_rotation % 360
            predicted_angle_info = f"{normalized_rotation:.1f} "
            angle_source = "ocr_text"
            print(f"Using OCR text rotation for '{label}' (Box {i}): {predicted_angle_info}")


    if angle_source == "none" and label in angle_models:
        angle_model = angle_models[label]
        num_classes = model_configs[label]["num_classes"]
        angle_class = predict_angle(cropped_pil_image, angle_model)
        angle_source = "resnet_model"

        if label == 'Pentagon':
            if angle_class == 0:
                predicted_angle_info = "0 "
            elif angle_class == 1:
                predicted_angle_info = "180 "
            else:
                predicted_angle_info = f"Pentagon Class {angle_class}"
        elif label == 'rectangle':
            x1, y1, x2, y2 = original_bbox
            width = x2 - x1
            height = y2 - y1
            if abs(width - height) < 5:
                predicted_angle_info = "0 "
            elif height >= 2 * width:
                predicted_angle_info = "90 "
            else:
                predicted_angle_info = "0 "
        elif label == 'Racetrack':
            racetrack_angles = {0: "45 ", 1: "90 ", 2: "135 ", 3: "0 "}
            predicted_angle_info = racetrack_angles.get(angle_class, f"Racetrack Class {angle_class}")
        elif label == 'Triangle':
            if angle_class == 0:
                predicted_angle_info = "0 "
            elif angle_class == 2:
                predicted_angle_info = "180 "
            else:
                if "triangle_aux" in angle_models:
                    aux_class = predict_angle(cropped_pil_image, angle_models["triangle_aux"])
                    if aux_class == 0:
                        predicted_angle_info = "90  (rightward tip)"
                    elif aux_class == 1:
                        predicted_angle_info = "270  (leftward tip)"
                    else:
                        predicted_angle_info = f"Triangle Auxiliary Class {aux_class} (unknown direction)"
                else:
                    predicted_angle_info = "Triangle auxiliary weights not found."
        elif label in ['arrow']:
            arrow_angles = {0: "0 ", 1: "90 ", 2: "180 ", 3: "270 "}
            predicted_angle_info = arrow_angles.get(angle_class, f"{label.capitalize()} Class {angle_class}")
        else:
            if num_classes == 8:
                angle_class_to_degrees_8_classes = {
                    0: "0 ", 1: "45 ", 2: "90 ", 3: "135 ",
                    4: "180 ", 5: "225 ", 6: "270 ", 7: "315 "
                }
                predicted_angle_info = angle_class_to_degrees_8_classes.get(angle_class, f"Class {angle_class} (Unknown Degree)")
            elif num_classes == 4:
                generic_4_class_angles = {
                    0: "0 ", 1: "90 ", 2: "180 ", 3: "270 "
                }
                predicted_angle_info = generic_4_class_angles.get(angle_class, f"Class {angle_class} (Unknown Degree)")
            elif num_classes == 2:
                generic_2_class_angles = {
                    0: "0 ", 1: "180 "
                }
                predicted_angle_info = generic_2_class_angles.get(angle_class, f"Class {angle_class} (Unknown Degree)")
            else:
                predicted_angle_info = f"Class {angle_class}"
    elif angle_source == "none":
        print(f"No angle model available for '{label}' (Box {i}). Skipping angle prediction.")

    predicted_angles.append({
        "angle": predicted_angle_info,
        "source": angle_source
    })
    print(f"Predicted Angle for '{label}' (Box {i}): {predicted_angle_info} (source: {angle_source})")
```

    UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
    UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.

    Loaded angle model for rectangle from /content/angle-models-weights/final_rectangle.pth
    Loaded angle model for Triangle from /content/angle-models-weights/Triangle.pth
    Loaded angle model for Racetrack from /content/angle-models-weights/final_racetrack.pth
    Loaded angle model for Pentagon from /content/angle-models-weights/best_resnet18_pentagon.pth
    Loaded angle model for arrow from /content/angle-models-weights/best_resnet18_arrow.pth
    Processed 6 bounding boxes.
    Received 6 sets of mask candidates.
    Applying occlusion correction to 'Racetrack' (Box 0).
    Using OCR text rotation for 'Racetrack' (Box 0): 249.2 
    Predicted Angle for 'Racetrack' (Box 0): 249.2  (source: ocr_text)
    Using OCR text rotation for 'Pentagon' (Box 1): 0.0 
    Predicted Angle for 'Pentagon' (Box 1): 0.0  (source: ocr_text)
    Applying occlusion correction to 'Racetrack' (Box 2).
    Using OCR text rotation for 'Racetrack' (Box 2): 319.0 
    Predicted Angle for 'Racetrack' (Box 2): 319.0  (source: ocr_text)
    Applying occlusion correction to 'Racetrack' (Box 3).
    Using OCR text rotation for 'Racetrack' (Box 3): 0.0 
    Predicted Angle for 'Racetrack' (Box 3): 0.0  (source: ocr_text)
    Applying occlusion correction to 'Racetrack' (Box 4).
    Using OCR text rotation for 'Racetrack' (Box 4): 357.4 
    Predicted Angle for 'Racetrack' (Box 4): 357.4  (source: ocr_text)
    Applying occlusion correction to 'Racetrack' (Box 5).
    Using OCR text rotation for 'Racetrack' (Box 5): 1.9 
    Predicted Angle for 'Racetrack' (Box 5): 1.9  (source: ocr_text)

# **Dominant Color Prediction**

``` python
import cv2
import numpy as np

def get_corner_background_color(image):

    h, w = image.shape[:2]
    s = min(5, w // 2, h // 2)

    tl = image[0:s, 0:s].reshape(-1, 3)
    tr = image[0:s, w-s:w].reshape(-1, 3)
    bl = image[h-s:h, 0:s].reshape(-1, 3)
    br = image[h-s:h, w-s:w].reshape(-1, 3)

    corners = np.vstack([tl, tr, bl, br])
    mean_bgr = np.mean(corners, axis=0)
    return mean_bgr

def find_smart_dominant_color(image, bg_color_bgr, k=4):

    try:
        lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        pixels = lab_image.reshape((-1, 3)).astype(np.float32)

        if pixels.shape[0] < 10:
            return None

        K = min(k, max(1, pixels.shape[0] // 200))
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)
        _, labels, centers = cv2.kmeans(
            pixels, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS
        )

        counts = np.bincount(labels.flatten())

        bg_color_uint8 = np.array([[bg_color_bgr]], dtype=np.uint8)
        bg_lab = cv2.cvtColor(bg_color_uint8, cv2.COLOR_BGR2LAB)[0][0].astype(np.float32)

        sorted_indices = np.argsort(counts)[::-1]

        final_color_bgr = None

        for idx in sorted_indices:
            center_lab = centers[idx]

            dist = np.linalg.norm(center_lab - bg_lab)

            if dist > 15:
                color_lab = center_lab.astype("uint8").reshape(1, 1, 3)
                final_color_bgr = cv2.cvtColor(color_lab, cv2.COLOR_LAB2BGR).reshape(3)
                break

        if final_color_bgr is None:
            color_lab = centers[sorted_indices[0]].astype("uint8").reshape(1, 1, 3)
            final_color_bgr = cv2.cvtColor(color_lab, cv2.COLOR_LAB2BGR).reshape(3)

        return tuple(int(c) for c in final_color_bgr[::-1])
    except Exception as e:
        return None

def rgb_to_hex(rgb_tuple):
    if rgb_tuple is None:
        return None
    r, g, b = rgb_tuple
    return "#{:02x}{:02x}{:02x}".format(
        max(0, min(255, int(r))),
        max(0, min(255, int(g))),
        max(0, min(255, int(b)))
    )

def iou(boxA, boxB):
    ax1,ay1,ax2,ay2 = boxA
    bx1,by1,bx2,by2 = boxB
    inter_x1 = max(ax1,bx1); inter_y1 = max(ay1,by1)
    inter_x2 = min(ax2,bx2); inter_y2 = min(ay2,by2)
    inter_w = max(0, inter_x2 - inter_x1); inter_h = max(0, inter_y2 - inter_y1)
    inter_area = inter_w * inter_h
    areaA = max(0, ax2-ax1) * max(0, ay2-ay1)
    areaB = max(0, bx2-bx1) * max(0, by2-by1)
    denom = areaA + areaB - inter_area
    return inter_area/denom if denom > 0 else 0.0

def greedy_iou_match(detections, angle_source, predicted_angles, iou_threshold=0.4):
    if angle_source is None or predicted_angles is None:
        return {}

    S = min(len(angle_source), len(predicted_angles))
    if S == 0:
        return {}

    D = len(detections)
    det_labels, det_boxes = [], []
    for d in detections:
        det_labels.append(d[0])
        p1,p2 = d[1], d[2]
        det_boxes.append((int(round(p1[0])), int(round(p1[1])), int(round(p2[0])), int(round(p2[1]))))

    src_labels, src_boxes = [], []
    for j in range(S):
        s = angle_source[j]
        src_labels.append(s[0])
        sp1, sp2 = s[1], s[2]
        src_boxes.append((int(round(sp1[0])), int(round(sp1[1])), int(round(sp2[0])), int(round(sp2[1]))))

    iou_mat = np.zeros((D, S), dtype=float)
    for i in range(D):
        for j in range(S):
            if det_labels[i] != src_labels[j]:
                continue
            iou_mat[i, j] = iou(det_boxes[i], src_boxes[j])

    matches = {}
    used_det = set(); used_src = set()
    while True:
        if iou_mat.size == 0:
            break
        idx = np.unravel_index(np.argmax(iou_mat), iou_mat.shape)
        max_iou = iou_mat[idx]
        if max_iou < iou_threshold:
            break
        di, sj = int(idx[0]), int(idx[1])
        if di in used_det or sj in used_src:
            iou_mat[di, sj] = 0.0
            continue
        matches[di] = predicted_angles[sj]
        used_det.add(di); used_src.add(sj)
        iou_mat[di, :] = 0.0
        iou_mat[:, sj] = 0.0

    return matches

def process_image_and_detections_simple(image_path, detections, predicted_angles=None, angle_source=None, iou_threshold=0.4):
    img = cv2.imread(image_path)
    if img is None:
        raise FileNotFoundError(f"Image not found at path: {image_path}")
    H, W = img.shape[:2]

    thin_line_labels = {"dashed-arrow", "doted-arrow", "connector"}

    allowed_angle_labels = {"arrow", "rectangle", "Racetrack", "Pentagon", "Triangle"}

    matches = greedy_iou_match(detections, angle_source, predicted_angles, iou_threshold=iou_threshold)

    out = []
    for i, det in enumerate(detections):
        if len(det) >= 4:
            label, p1, p2, existing_color = det[0], det[1], det[2], det[3]
        else:
            label, p1, p2 = det[0], det[1], det[2]
            existing_color = None

        x1 = max(0, int(round(float(p1[0])))); y1 = max(0, int(round(float(p1[1]))))
        x2 = min(W, int(round(float(p2[0])))); y2 = min(H, int(round(float(p2[1]))))

        if x2 <= x1 or y2 <= y1:
            dom_hex = existing_color
        else:
            full_crop = img[y1:y2, x1:x2]

            if full_crop is None or full_crop.size == 0:
                dom_hex = existing_color
            else:

                bg_color_bgr = get_corner_background_color(full_crop)

                h_c, w_c = full_crop.shape[:2]
                margin_h = int(h_c * 0.20)
                margin_w = int(w_c * 0.20)

                if margin_h > 0 and margin_w > 0:
                    center_crop = full_crop[margin_h:h_c-margin_h, margin_w:w_c-margin_w]
                else:
                    center_crop = full_crop

                dom_rgb = find_smart_dominant_color(center_crop, bg_color_bgr)
                dom_hex = existing_color if existing_color else rgb_to_hex(dom_rgb)

        angle_val = None
        if label in allowed_angle_labels:
            if i in matches:
                angle_val = matches[i]
            elif predicted_angles is not None and i < len(predicted_angles):
                angle_val = predicted_angles[i]

        out.append((label, (x1, y1), (x2, y2), dom_hex, angle_val))

    return out

try:
    detections_input = detections
except NameError:
    try:
        detections_input = detections_list
    except NameError:
        raise RuntimeError("Set 'detections' or 'detections_list' before running this cell.")

preds = globals().get('predicted_angles', None)
src = globals().get('filtered_detections', None)

final_detections = process_image_and_detections_simple(file_path, detections_input, predicted_angles=preds, angle_source=src, iou_threshold=0.4)


print("detections = [")
for det in final_detections:
    label, (x1, y1), (x2, y2), color_hex, angle_info = det
    print(f"    ('{label}', ({x1}, {y1}), ({x2}, {y2}), {repr(color_hex)}, {repr(angle_info)}),")
print("]")
```

    detections = [
        ('Racetrack', (525, 337), (773, 732), '#5f803e', {'angle': '249.2 ', 'source': 'ocr_text'}),
        ('circle', (75, 354), (211, 486), '#ba6c26', None),
        ('Pentagon', (198, 193), (629, 608), '#263619', {'angle': '0.0 ', 'source': 'ocr_text'}),
        ('Racetrack', (399, 37), (734, 357), '#faf9e7', {'angle': '319.0 ', 'source': 'ocr_text'}),
        ('circle', (191, 625), (328, 758), '#ba6c26', None),
        ('circle', (540, 567), (676, 700), '#ba6c26', None),
        ('Racetrack', (91, 35), (442, 352), '#6d7a4b', {'angle': '0.0 ', 'source': 'ocr_text'}),
        ('circle', (580, 198), (715, 331), '#ba6c26', None),
        ('circle', (290, 64), (423, 196), '#ba6c26', None),
        ('Racetrack', (182, 610), (573, 773), '#dcaa73', {'angle': '357.4 ', 'source': 'ocr_text'}),
        ('Racetrack', (58, 338), (307, 737), '#c8945a', {'angle': '1.9 ', 'source': 'ocr_text'}),
    ]

# **Final JSON Output**

``` python
import numpy as np
import json
from PIL import Image

def create_nodes_json(shape_detections):

    diagram_nodes = []
    for i, det in enumerate(shape_detections):
        label = det[0] if len(det) > 0 else ""
        top_left = det[1] if len(det) > 1 else (0, 0)
        bottom_right = det[2] if len(det) > 2 else (0, 0)
        color = det[3] if len(det) > 3 else None
        angle = det[4] if len(det) > 4 else None

        if angle is None:
            angle_val = ""
        elif isinstance(angle, dict):
            raw = angle.get("angle", "")
            if isinstance(raw, str):
                angle_val = raw.strip()
            else:
                angle_val = str(raw) if raw is not None else ""
        elif isinstance(angle, str):
            angle_val = angle.strip()
            if angle_val.lower() == "none":
                angle_val = ""
        else:
            # It's a number
            angle_val = str(angle)

        x1, y1 = top_left
        x2, y2 = bottom_right
        center_x = int(round((x1 + x2) / 2))
        center_y = int(round((y1 + y2) / 2))
        width = int(round(x2 - x1))
        height = int(round(y2 - y1))

        node = {
            "id": f"node{i+1}",
            "x": center_x,
            "y": center_y,
            "text": "",
            "shape": label,
            "color": color,
            "width": width,
            "height": height,
            "angle": angle_val
        }
        diagram_nodes.append(node)
    return diagram_nodes

def _det_to_center(det):
    if det is None:
        return None
    label = det[0] if len(det) > 0 else ""
    tl = det[1] if len(det) > 1 else (0, 0)
    br = det[2] if len(det) > 2 else (0, 0)
    cx = int(round((tl[0] + br[0]) / 2))
    cy = int(round((tl[1] + br[1]) / 2))
    return (label, cx, cy)

def _find_node_by_det(diagram_nodes, det, tol=8):
    if det is None:
        return None
    label, cx, cy = _det_to_center(det)
    for node in diagram_nodes:
        if node['shape'] == label:
            if abs(node['x'] - cx) <= tol and abs(node['y'] - cy) <= tol:
                return node['id']
    best = (None, 1e9)
    for node in diagram_nodes:
        if node['shape'] == label:
            dist = (node['x']-cx)**2 + (node['y']-cy)**2
            if dist < best[1]:
                best = (node['id'], dist)
    return best[0]

def create_edges_json(arrow_connections, diagram_nodes, all_detections, arrowhead_radius=50):
    diagram_edges = []
    arrow_head_centers = []
    for d in [dd for dd in all_detections if dd[0] == "arrow_head"]:
        tl = d[1] if len(d) > 1 else (0, 0)
        br = d[2] if len(d) > 2 else (0, 0)
        arrow_head_centers.append(((tl[0]+br[0])/2.0, (tl[1]+br[1])/2.0))

    for i, conn in enumerate(arrow_connections):
        tail_det = conn.get("tail_connected_to")
        head_det = conn.get("head_connected_to")
        src_id = _find_node_by_det(diagram_nodes, tail_det)
        tgt_id = _find_node_by_det(diagram_nodes, head_det)
        if not src_id or not tgt_id:
            continue

        head_pt = tuple(conn.get('head')) if conn.get('head') is not None else None
        tail_pt = tuple(conn.get('tail')) if conn.get('tail') is not None else None

        def _has_arrow_at(pt):
            if pt is None:
                return False
            for ah in arrow_head_centers:
                if np.linalg.norm(np.array(pt) - np.array(ah)) < arrowhead_radius:
                    return True
            return False

        has_at_tail = _has_arrow_at(tail_pt)
        has_at_head = _has_arrow_at(head_pt)

        startArrow = False
        endArrow = False

        if has_at_tail and has_at_head:
            startArrow = True; endArrow = True
        elif has_at_tail and not has_at_head:
            src_id, tgt_id = tgt_id, src_id; startArrow = False; endArrow = True
        elif has_at_head and not has_at_tail:
            startArrow = False; endArrow = True
        else:
            if head_pt and tail_pt:
                x_diff = head_pt[0] - tail_pt[0]
                y_diff = head_pt[1] - tail_pt[1]
                if abs(x_diff) > abs(y_diff):
                    if x_diff < 0:
                        src_id, tgt_id = tgt_id, src_id
                else:
                    if y_diff < 0:
                        src_id, tgt_id = tgt_id, src_id

        raw_label = conn.get("original_label") or conn.get("label") or ""
        line_style = raw_label.split('-')[0] if raw_label else "solid"

        edge = {
            "id": f"edge{i+1}",
            "source": src_id,
            "target": tgt_id,
            "lineStyle": line_style,
            "startArrow": bool(startArrow),
            "endArrow": bool(endArrow),
            "color": "#333333"
        }
        diagram_edges.append(edge)

    return diagram_edges

def map_ocr_to_nodes(diagram_nodes, processed_ocr_data, relaxation_pixels=15, max_area_ratio=80):

    nodes = [n.copy() for n in diagram_nodes]
    ocr_boxes = []
    if not processed_ocr_data:
        return nodes, []

    for ocr in processed_ocr_data:
        bbox = ocr.get('bbox')
        text = ocr.get('text', "")
        if not bbox or not text.strip():
            continue

        ocr_x1, ocr_y1, ocr_x2, ocr_y2 = bbox
        ocr_w = max(1, ocr_x2 - ocr_x1)
        ocr_h = max(1, ocr_y2 - ocr_y1)
        ocr_area = ocr_w * ocr_h

        candidates = []
        for idx, node in enumerate(nodes):
            cx, cy, w, h = node['x'], node['y'], node['width'], node['height']
            node_x1 = cx - (w // 2) - relaxation_pixels
            node_y1 = cy - (h // 2) - relaxation_pixels
            node_x2 = cx + (w // 2) + relaxation_pixels
            node_y2 = cy + (h // 2) + relaxation_pixels

            if (ocr_x1 >= node_x1 and ocr_y1 >= node_y1 and ocr_x2 <= node_x2 and ocr_y2 <= node_y2):
                node_area = max(1, w * h)
                area_ratio = node_area / float(ocr_area)
                dx = cx - (ocr_x1 + ocr_x2) / 2.0
                dy = cy - (ocr_y1 + ocr_y2) / 2.0
                center_dist2 = dx * dx + dy * dy
                candidates.append((idx, node_area, area_ratio, center_dist2))

        if candidates:
            filtered = [c for c in candidates if c[2] <= max_area_ratio]
            chosen = min(filtered if filtered else candidates, key=lambda t: (t[1], t[3]))
            best_idx = chosen[0]
            node = nodes[best_idx]
            node['text'] = (node.get('text', "") + (" " if node.get('text') else "") + text).strip()
        else:
            ocr_boxes.append({
                "text": text,
                "x1": int(ocr_x1), "y1": int(ocr_y1),
                "x2": int(ocr_x2), "y2": int(ocr_y2)
            })

    standalone_text_labels = []
    merge_y_thresh = 18
    merge_x_thresh = 40
    if ocr_boxes:
        ocr_boxes = sorted(ocr_boxes, key=lambda b: (b['y1'], b['x1']))
        used = [False] * len(ocr_boxes)
        for i, box in enumerate(ocr_boxes):
            if used[i]:
                continue
            texts = [box['text'].strip()]
            x1, y1, x2, y2 = box['x1'], box['y1'], box['x2'], box['y2']
            used[i] = True
            for j in range(i + 1, len(ocr_boxes)):
                if used[j]:
                    continue
                ob = ocr_boxes[j]
                ob_x1, ob_y1, ob_x2, ob_y2 = ob['x1'], ob['y1'], ob['x2'], ob['y2']
                vertical_close = abs(ob_y1 - y2) <= merge_y_thresh or abs(ob_y2 - y1) <= merge_y_thresh
                horizontal_overlap = (ob_x1 <= x2 + merge_x_thresh and ob_x2 >= x1 - merge_x_thresh)
                if vertical_close and horizontal_overlap:
                    texts.append(ob['text'].strip())
                    x1 = min(x1, ob_x1)
                    y1 = min(y1, ob_y1)
                    x2 = max(x2, ob_x2)
                    y2 = max(y2, ob_y2)
                    used[j] = True

            standalone_text_labels.append({
                "id": f"text{len(standalone_text_labels) + 1}",
                "x": int(round((x1 + x2) / 2)),
                "y": int(round((y1 + y2) / 2)),
                "text": " ".join(texts).strip(),
                "bbox": {"x1": x1, "y1": y1, "x2": x2, "y2": y2},
                "width": x2 - x1,
                "height": y2 - y1
            })

    return nodes, standalone_text_labels


# ---- Build final JSON ----
source_detections = None
if 'final_detections' in globals() and isinstance(globals()['final_detections'], list):
    source_detections = globals()['final_detections']
elif 'final_detections' in globals() and globals()['final_detections'] is None:
    source_detections = None
elif 'detections' in globals() and isinstance(globals()['detections'], list):
    source_detections = globals()['detections']
elif 'detections_list' in globals() and isinstance(globals()['detections_list'], list):
    source_detections = globals()['detections_list']
else:
    raise RuntimeError("No detections available. Ensure `final_detections` or `detections` is defined in scope.")

shape_detections = [d for d in source_detections if d[0] not in ["dashed-arrow", "dotted-arrow", "solid-arrow", "arrow_head"]]
nodes_json = create_nodes_json(shape_detections)

if 'arrow_connections' not in globals():
    arrow_connections = []
edges_json = create_edges_json(arrow_connections, nodes_json, source_detections)

standalone_text_labels = []
if 'final_processed_data' in globals() and final_processed_data:
    nodes_json, standalone_text_labels = map_ocr_to_nodes(
        nodes_json, final_processed_data, relaxation_pixels=15,
)

if 'image' not in globals():
    raise RuntimeError("`image` not found in scope. Provide `image` (PIL Image or numpy array).")
img_obj = globals()['image']
if hasattr(img_obj, "shape"):
    canvas_w, canvas_h = int(img_obj.shape[1]), int(img_obj.shape[0])
else:
    try:
        canvas_w, canvas_h = int(img_obj.size[0]), int(img_obj.size[1])
    except Exception as e:
        raise RuntimeError("Cannot determine canvas size from `image`. Provide a numpy array or PIL Image.") from e

final_json_output = {
    "canvas": {"width": canvas_w, "height": canvas_h},
    "nodes": nodes_json,
    "edges": edges_json,
    "text_labels": standalone_text_labels
}

print("\n--- Final JSON Output ---")
print(json.dumps(final_json_output, indent=2))
```


    --- Final JSON Output ---
    {
      "canvas": {
        "width": 801,
        "height": 798
      },
      "nodes": [
        {
          "id": "node1",
          "x": 649,
          "y": 534,
          "text": "Hope is a dangerous illusion",
          "shape": "Racetrack",
          "color": "#5f803e",
          "width": 248,
          "height": 395,
          "angle": "249.2"
        },
        {
          "id": "node2",
          "x": 143,
          "y": 420,
          "text": "#5",
          "shape": "circle",
          "color": "#ba6c26",
          "width": 136,
          "height": 132,
          "angle": ""
        },
        {
          "id": "node3",
          "x": 414,
          "y": 400,
          "text": "Fragments of Fire: Short Truths for a Heavy Mind",
          "shape": "Pentagon",
          "color": "#263619",
          "width": 431,
          "height": 415,
          "angle": "0.0"
        },
        {
          "id": "node4",
          "x": 566,
          "y": 197,
          "text": "The void always stares back.",
          "shape": "Racetrack",
          "color": "#faf9e7",
          "width": 335,
          "height": 320,
          "angle": "319.0"
        },
        {
          "id": "node5",
          "x": 260,
          "y": 692,
          "text": "#4",
          "shape": "circle",
          "color": "#ba6c26",
          "width": 137,
          "height": 133,
          "angle": ""
        },
        {
          "id": "node6",
          "x": 608,
          "y": 634,
          "text": "#3",
          "shape": "circle",
          "color": "#ba6c26",
          "width": 136,
          "height": 133,
          "angle": ""
        },
        {
          "id": "node7",
          "x": 266,
          "y": 194,
          "text": "We rot beautifully.",
          "shape": "Racetrack",
          "color": "#6d7a4b",
          "width": 351,
          "height": 317,
          "angle": "0.0"
        },
        {
          "id": "node8",
          "x": 648,
          "y": 264,
          "text": "#2",
          "shape": "circle",
          "color": "#ba6c26",
          "width": 135,
          "height": 133,
          "angle": ""
        },
        {
          "id": "node9",
          "x": 356,
          "y": 130,
          "text": "#1",
          "shape": "circle",
          "color": "#ba6c26",
          "width": 133,
          "height": 132,
          "angle": ""
        },
        {
          "id": "node10",
          "x": 378,
          "y": 692,
          "text": "Light lies; shadows remember.",
          "shape": "Racetrack",
          "color": "#dcaa73",
          "width": 391,
          "height": 163,
          "angle": "357.4"
        },
        {
          "id": "node11",
          "x": 182,
          "y": 538,
          "text": "Especially you. Everything lends.",
          "shape": "Racetrack",
          "color": "#c8945a",
          "width": 249,
          "height": 399,
          "angle": "1.9"
        }
      ],
      "edges": [],
      "text_labels": []
    }

# **MAP**

    # This is formatted as code

**Mean Average Precision of the RF-DETR model**

``` python
import supervision as sv
from rfdetr import RFDETRMedium
from PIL import Image
from supervision.metrics.mean_average_precision import MeanAveragePrecision

images_directory_path = "/content/Dataset/valid"
annotations_path = "/content/Dataset/valid/_annotations.coco.json"
preTrainedWeights = "/content/weights/checkpoint_best_regular.pth"

model = RFDETRMedium(pretrain_weights=preTrainedWeights)
model.optimize_for_inference()

test_dataset = sv.DetectionDataset.from_coco(
    images_directory_path=images_directory_path,
    annotations_path=annotations_path,
)


def model_callback(image: Image.Image) -> sv.Detections:
    return model.predict(image)

sticky_note_class_name = "Sticky Notes"
sticky_note_class_idx = None
for idx, cls_name in enumerate(test_dataset.classes):
    if cls_name.lower() == sticky_note_class_name.lower():
        sticky_note_class_idx = idx
        break

map_metric = MeanAveragePrecision()
all_predictions = []
all_ground_truths = []
for path, image, ground_truth in test_dataset:
    preds = model_callback(image)
    if sticky_note_class_idx is not None:
        preds = preds[preds.class_id != sticky_note_class_idx]
        ground_truth = ground_truth[ground_truth.class_id != sticky_note_class_idx]
    all_predictions.append(preds)
    all_ground_truths.append(ground_truth)

map_metric.update(all_predictions, all_ground_truths)

result = map_metric.compute()

print(f"Overall mAP@0.5:0.95 = {result.map50_95:.4f}")
print(f"mAP@0.5 = {result.map50:.4f}")
print("Number of classes in dataset:", len(test_dataset.classes))
print("Number of classes with AP computed:", result.ap_per_class.shape[0])

print("Per-class AP:")
for i, class_idx in enumerate(result.matched_classes):
    if sticky_note_class_idx is not None and class_idx == sticky_note_class_idx:
        continue
    class_name = test_dataset.classes[class_idx]
    ap_scores = result.ap_per_class[i]
    mean_ap = ap_scores.mean()
    print(f"{class_name}: {mean_ap:.4f}")
```

    Using a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.
    Using patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.
    Loading pretrain weights

    WARNING:rfdetr.main:num_classes mismatch: pretrain weights has 16 classes, but your model has 90 classes
    reinitializing detection head with 16 classes
    `loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
    TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
    TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
    TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
    UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
    TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).

    Overall mAP@0.5:0.95 = 0.8265
    mAP@0.5 = 0.9478
    Number of classes in dataset: 17
    Number of classes with AP computed: 15
    Per-class AP:
    Cloud: 0.8402
    Diamond: 0.8372
    Double Arrow: 0.9713
    Pentagon: 0.9066
    Racetrack: 0.9020
    Star: 0.9215
    Triangle: 0.9800
    arrow: 0.9016
    arrow_head: 0.5326
    circle: 0.8608
    dashed-arrow: 0.6638
    dotted-arrow: 0.6027
    rectangle: 0.8934
    rounded rectangle: 0.8837
    solid-arrow: 0.6995

**Verifying the Arrow connections for finding the algorithm accuracy**

``` python
import json
import matplotlib.pyplot as plt


json_data = final_json_output

# --- Parse JSON ---
data = json_data
nodes = {node["id"]: node for node in data["nodes"]}
edges = data["edges"]
# --- Setup Canvas ---
fig, ax = plt.subplots(figsize=(data["canvas"]["width"]/100, data["canvas"]["height"]/100))
ax.set_xlim(0, data["canvas"]["width"])
ax.set_ylim(0, data["canvas"]["height"])
ax.invert_yaxis()
ax.axis("off")

for node in nodes.values():
    label = node["text"] if node["text"] else node["shape"]
    ax.text(node["x"], node["y"], label, ha="center", va="center", fontsize=8, color="black", wrap=True)
for edge in edges:
    src = nodes[edge["source"]]
    tgt = nodes[edge["target"]]
    x1, y1 = src["x"], src["y"]
    x2, y2 = tgt["x"], tgt["y"]
    ax.annotate("",
                xy=(x2, y2), xytext=(x1, y1),
                arrowprops=dict(arrowstyle="->", color="gray", lw=1.2))
plt.tight_layout()
plt.show()
```

![](00_core.backup_files/figure-commonmark/cell-24-output-1.png)

``` python
!pip install pandas
```

    Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)
    Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)
    Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)
    Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)
    Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)

Confusion metrix on the Output of the algorithm of arrow connections

``` python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def clean_and_extract_number(value):

    if pd.isna(value):
        return 0
    s_value = str(value).strip()
    parts = s_value.split(' ', 1)
    numeric_part = parts[0]

    try:
        return int(float(numeric_part))
    except (ValueError, TypeError):
        return 0

def create_confusion_matrix(file_path):

    try:
        df = pd.read_excel(file_path)
        df.columns = df.columns.str.strip()

        numeric_columns = ['TOTAL ARROW', 'CORECT ARROW NUMBER', 'WRONG ARROW NUMBER']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = df[col].apply(clean_and_extract_number)
            else:
                print(f"Warning: Column '{col}' not found. Please check spelling in your Excel file.")

        true_positives = df['CORECT ARROW NUMBER'].sum()
        false_positives = df['WRONG ARROW NUMBER'].sum()
        false_negatives = (df['TOTAL ARROW'] - df['CORECT ARROW NUMBER']).sum()

        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0

        print("----------------------------------------")
        print("   Algorithm Performance Metrics")
        print("----------------------------------------")
        print(f"True Positives (TP):  {true_positives}")
        print(f"False Positives (FP): {false_positives}")
        print(f"False Negatives (FN): {false_negatives}")
        print("True Negatives (TN):  Zero 0.")
        print("\n--- Key Metrics ---")
        print(f"Precision: {precision:.2%}")
        print(f"Recall (Sensitivity): {recall:.2%}")
        print("----------------------------------------\n")

        matrix_data = [[true_positives, false_negatives], [false_positives, 0]]

        matrix_labels = [
            [f"{true_positives}\n(TP)", f"{false_negatives}\n(FN)"],
            [f"{false_positives}\n(FP)", "Zero (0) \n(TN)"]
        ]

        plt.figure(figsize=(8, 6))
        sns.heatmap(matrix_data, annot=matrix_labels, fmt="", cmap='Blues',
                    xticklabels=['Predicted: Arrow', 'Predicted: Not Arrow'],
                    yticklabels=['Actual: Arrow', 'Actual: Not Arrow'],
                    cbar=False, annot_kws={"size": 14})

        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.title('Confusion Matrix for Arrow Detection', fontsize=16)

        # This command displays the plot
        plt.show()

    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == '__main__':
    file_path = '/content/REPORT.xlsx'
    create_confusion_matrix(file_path)
```

    ----------------------------------------
       Algorithm Performance Metrics
    ----------------------------------------
    True Positives (TP):  294
    False Positives (FP): 10
    False Negatives (FN): 10
    True Negatives (TN):  Zero 0.

    --- Key Metrics ---
    Precision: 96.71%
    Recall (Sensitivity): 96.71%
    ----------------------------------------

![](00_core.backup_files/figure-commonmark/cell-26-output-2.png)

correct

``` python
import json
import math
import textwrap
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont

class DiagramGenerator:
    def __init__(self, json_data):
        self.data = json_data
        self.canvas_w = json_data['canvas']['width']
        self.canvas_h = json_data['canvas']['height']
        self.image = Image.new('RGB', (self.canvas_w, self.canvas_h), color='white')
        self.draw = ImageDraw.Draw(self.image)

        try:
            self.font = ImageFont.truetype("arial.ttf", 14)
            self.font_bold = ImageFont.truetype("arialbd.ttf", 13)
        except IOError:
            self.font = ImageFont.load_default()
            self.font_bold = ImageFont.load_default()

    def generate_image(self):
        nodes = self.data.get('nodes', [])


        sorted_nodes = sorted(nodes, key=lambda n: n['width'] * n['height'], reverse=True)

        for node in sorted_nodes:
            self.draw_node(node)

        for label in self.data.get('text_labels', []):
            self.draw_label(label)

        return self.image

    def get_bbox(self, node):
        cx, cy = node['x'], node['y']
        w, h = node['width'], node['height']
        return (cx - w/2, cy - h/2, cx + w/2, cy + h/2)

    def get_poly_points(self, cx, cy, w, h, shape_type):
        if shape_type == "diamond":
            return [(0, -h/2), (w/2, 0), (0, h/2), (-w/2, 0)]
        elif shape_type == "triangle":
            return [(0, -h/2), (w/2, h/2), (-w/2, h/2)]
        elif shape_type == "pentagon":
            return self._calculate_ngon(5, w/2, h/2)
        elif "arrow" in shape_type and "double" not in shape_type:
            head_len = w * 0.4
            return [(-w/2, -h/4), (w/2 - head_len, -h/4), (w/2 - head_len, -h/2),
                    (w/2, 0), (w/2 - head_len, h/2), (w/2 - head_len, h/4), (-w/2, h/4)]
        elif "double arrow" in shape_type:
            head_w = w * 0.2
            shaft_h = h * 0.4
            return [(-w/2, 0), (-w/2 + head_w, -h/2), (-w/2 + head_w, -shaft_h/2),
                    (w/2 - head_w, -shaft_h/2), (w/2 - head_w, -h/2), (w/2, 0),
                    (w/2 - head_w, h/2), (w/2 - head_w, shaft_h/2),
                    (-w/2 + head_w, shaft_h/2), (-w/2 + head_w, h/2)]
        return [(-w/2, -h/2), (w/2, -h/2), (w/2, h/2), (-w/2, h/2)]

    def draw_node(self, node):
        raw_shape = node.get('shape', 'rectangle')
        shape_type = raw_shape.lower().replace("_", " ").strip()

        cx, cy = node['x'], node['y']
        w, h = node['width'], node['height']

        color = node.get('color', '#cccccc')
        if len(color) == 9 and color.startswith('#'): color = color[:7]

        try: angle = float(node.get('angle', 0))
        except: angle = 0

        text = node.get('text', '')

        x1, y1, x2, y2 = self.get_bbox(node)

        if shape_type in ["circle", "ellipse", "start", "end"]:
            self.draw.ellipse([x1, y1, x2, y2], fill=color)
        elif shape_type == "racetrack" or shape_type == "terminal":
            self.draw.rounded_rectangle([x1, y1, x2, y2], radius=h/2, fill=color)
        elif shape_type in ["rounded rectangle", "process"]:
            self.draw.rounded_rectangle([x1, y1, x2, y2], radius=15, fill=color)
        elif shape_type == "cloud":
            self.draw.ellipse([x1, y1, x2, y2], fill=color)
        elif shape_type == "star":
            points = self._calculate_star_points(5, w/2, w/4)
            self._draw_rotated_polygon(points, cx, cy, angle, color)
        else:
            points = self.get_poly_points(cx, cy, w, h, shape_type)
            self._draw_rotated_polygon(points, cx, cy, angle, color)

        if text:
            self._draw_text_centered(text, cx, cy, w, h)

    def draw_label(self, label):
        self.draw.text((label['x'], label['y']), label.get('text', ''), font=self.font, fill="black")

    def _rotate_point(self, point, angle_deg):
        angle_rad = math.radians(angle_deg)
        x, y = point
        return (x * math.cos(angle_rad) - y * math.sin(angle_rad),
                x * math.sin(angle_rad) + y * math.cos(angle_rad))

    def _draw_rotated_polygon(self, points, cx, cy, angle, color):
        rotated = []
        for p in points:
            rx, ry = self._rotate_point(p, angle)
            rotated.append((cx + rx, cy + ry))
        self.draw.polygon(rotated, fill=color)

    def _calculate_ngon(self, sides, radius_x, radius_y):
        points = []
        for i in range(sides):
            angle = (2 * math.pi * i) / sides - (math.pi / 2)
            points.append((math.cos(angle) * radius_x, math.sin(angle) * radius_y))
        return points

    def _calculate_star_points(self, points_count, outer_radius, inner_radius):
        points = []
        angle_step = math.pi / points_count
        current_angle = -math.pi / 2
        for i in range(points_count * 2):
            radius = outer_radius if i % 2 == 0 else inner_radius
            points.append((math.cos(current_angle) * radius, math.sin(current_angle) * radius))
            current_angle += angle_step
        return points

    def _draw_text_centered(self, text, cx, cy, w, h):
        if not text: return

        char_w_approx = 6.5
        padding = 10
        max_chars = max(1, int((w - padding) / char_w_approx))

        lines = textwrap.wrap(text, width=max_chars)

        line_height = 14
        total_text_h = len(lines) * line_height

        current_y = cy - (total_text_h / 2)

        for line in lines:
            bbox = self.draw.textbbox((0, 0), line, font=self.font_bold)
            text_w = bbox[2] - bbox[0]
            text_x = cx - (text_w / 2)
            self.draw.text((text_x, current_y), line, font=self.font_bold, fill="black")
            current_y += line_height

if __name__ == "__main__":

    data = {
  "canvas": {
    "width": 801,
    "height": 798
  },
  "nodes": [
    {
      "id": "node1",
      "x": 649,
      "y": 534,
      "text": "Hope is a dangerous illusion",
      "shape": "Racetrack",
      "color": "#5f803e",
      "width": 248,
      "height": 395,
      "angle": {
        "angle": "290.7 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node2",
      "x": 143,
      "y": 420,
      "text": "#5",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 136,
      "height": 132,
      "angle": ""
    },
    {
      "id": "node3",
      "x": 414,
      "y": 400,
      "text": "Fragments of Fire: Short Truths for a Heavy Mind",
      "shape": "Pentagon",
      "color": "#263619",
      "width": 431,
      "height": 415,
      "angle": {
        "angle": "0.0 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node4",
      "x": 566,
      "y": 197,
      "text": "The void always stares back.",
      "shape": "Racetrack",
      "color": "#faf9e7",
      "width": 335,
      "height": 320,
      "angle": {
        "angle": "41.2 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node5",
      "x": 260,
      "y": 692,
      "text": "#4",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 137,
      "height": 133,
      "angle": ""
    },
    {
      "id": "node6",
      "x": 608,
      "y": 634,
      "text": "#3",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 136,
      "height": 133,
      "angle": ""
    },
    {
      "id": "node7",
      "x": 266,
      "y": 194,
      "text": "We rot beautifully.",
      "shape": "Racetrack",
      "color": "#6d7a4b",
      "width": 351,
      "height": 317,
      "angle": {
        "angle": "0.0 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node8",
      "x": 648,
      "y": 264,
      "text": "#2",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 135,
      "height": 133,
      "angle": ""
    },
    {
      "id": "node9",
      "x": 356,
      "y": 130,
      "text": "#1",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 133,
      "height": 132,
      "angle": ""
    },
    {
      "id": "node10",
      "x": 378,
      "y": 692,
      "text": "Light lies; shadows remember.",
      "shape": "Racetrack",
      "color": "#dcaa73",
      "width": 391,
      "height": 163,
      "angle": {
        "angle": "2.5 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node11",
      "x": 182,
      "y": 538,
      "text": "Especially you. Everything lends.",
      "shape": "Racetrack",
      "color": "#c8945a",
      "width": 249,
      "height": 399,
      "angle": {
        "angle": "178.0 ",
        "source": "ocr_text"
      }
    }
  ],
  "edges": [],
  "text_labels": []
}

    gen = DiagramGenerator(data)
    final_img = gen.generate_image()

    plt.figure(figsize=(10, 8))
    plt.imshow(final_img)
    plt.axis('off')
    plt.show()
```

![](00_core.backup_files/figure-commonmark/cell-27-output-1.png)
