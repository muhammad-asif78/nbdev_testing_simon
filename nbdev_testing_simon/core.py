# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/index.ipynb.

# %% auto 0
__all__ = ['dataset', 'img_path', 'preTrainedWeights', 'detected_objects', 'STANDARD_ANGLES', 'SNAP_TOLERANCE', 'detections',
           'include_labels', 'filtered_detections', 'boxes', 'model_cfg', 'sam2_checkpoint', 'sam2_model', 'predictor',
           'image', 'completable_shapes', 'occlusion_flags', 'device', 'angle_transform', 'model_configs',
           'angle_models', 'triangle_aux_model_path', 'predicted_angles', 'preds', 'src', 'final_detections',
           'source_detections', 'shape_detections', 'nodes_json', 'edges_json', 'standalone_text_labels', 'img_obj',
           'final_json_output', 'json_data', 'data', 'nodes', 'edges', 'fig', 'ax', 'download_drive_to_weights',
           'download_file_from_gdrive', 'download_and_extract_strip_top_folder', 'load_or_convert_file',
           'calculate_rotation_from_polygon', 'snap_to_standard_angle', 'get_text_rotation_for_shape',
           'process_paddleocr_json', 'plot_ocr_detections', 'show_image', 'crop_image_region', 'convert_to_binary_mask',
           'connect_dotted_simple_morph', 'find_farthest_points_in_contour', 'interpolate_dashed_arrows',
           'extract_skeleton', 'get_skeleton_graph_nodes', 'find_farthest_points_euclidean', 'check_bbox_intersection',
           'simple_bridge_by_interpolation', 'bbox_iou', 'pixel_level_ocr_interference', 'mask_overlapping_ocr_regions',
           'match_arrowhead_to_endpoint', 'bfs_get_path', 'find_best_arrow_by_straightness', 'skeleton_has_cycle',
           'find_farthest_geodesic_endpoints', 'dilate_ocr_regions', 'find_skeleton_points_on_bbox', 'debug_stage',
           'point_to_bbox_distance', 'nearest_point_on_bbox', 'find_nearest_shape_bbox', 'draw_connections_on_image',
           'establish_connections', 'check_overlap', 'predict_angle', 'get_corner_background_color',
           'find_smart_dominant_color', 'rgb_to_hex', 'iou', 'greedy_iou_match', 'process_image_and_detections_simple',
           'create_nodes_json', 'create_edges_json', 'map_ocr_to_nodes', 'clean_and_extract_number',
           'create_confusion_matrix', 'DiagramGenerator']

# %% ../nbs/index.ipynb 1
#| export

# %% ../nbs/index.ipynb 2
#| export

# %% ../nbs/index.ipynb 3
#| export

# %% ../nbs/index.ipynb 4
#| export

# %% ../nbs/index.ipynb 7
#| eval: false
#| hide: true
import re
import subprocess
from pathlib import Path

def download_drive_to_weights(link_or_id: str):
    """
    Download a Google Drive file or folder into the local 'weights' directory.

    This is intended for manual use, not for automated docs builds.

    Example usage:
        download_drive_to_weights("https://drive.google.com/drive/folders/17CXXFCGw5npR1cyrg082yq1IdcnH6-ms?usp=sharing")
    """
    dest = "weights"
    Path(dest).mkdir(parents=True, exist_ok=True)
    print(f"Using '{dest}' folder for downloads...")

    s = str(link_or_id).strip()

    # Try to parse a folder or file id
    m = re.search(r'folders/([A-Za-z0-9_-]+)', s) or re.search(r'id=([A-Za-z0-9_-]+)', s)
    if not m:
        m = re.search(r'/d/([A-Za-z0-9_-]+)', s) or re.search(r'([A-Za-z0-9_-]{10,})$', s)
    if not m:
        print("Couldn't parse an ID from the input. Please paste the folder link or the long '1...' id.")
        return
    fid = m.group(1)

    use_folder = ('folders' in s) or (len(fid) > 20)

    try:
        if use_folder:
            print("Would download Drive folder to ./weights ...")
            print(f"Example command:")
            print(f"  gdown --folder 'https://drive.google.com/drive/folders/{fid}' -O {dest!r}")
        else:
            print("Would download file to ./weights ...")
            url = f"https://drive.google.com/uc?id={fid}"
            print("Example command:")
            print(f"  gdown {url!r} -O {dest!r}")

        print("Done — check the local 'weights' folder after running the above command manually.")
    except subprocess.CalledProcessError as e:
        print("gdown failed:", e)

# %% ../nbs/index.ipynb 8
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 9
#| eval: false
#| hide: true
import re
import os
from pathlib import Path

def download_file_from_gdrive(link_or_id: str, dest_folder: str = "sam2_checkpoints", filename: str = None):
    """
    Helper to download a single SAM2 checkpoint file from Google Drive
    into the local 'sam2_checkpoints' folder.

    NOTE: This function is not executed during docs build (eval: false).

    Example usage:
        download_file_from_gdrive(
            "https://drive.google.com/file/d/11L0iHZ1Ktcjhd0vqKI_3-PJ3DTv2UyOc/view?usp=sharing",
            dest_folder='sam2_checkpoints',
            filename='sam2_hiera_base_plus.pt'
        )
    """
    # Parse the file id from a full Drive link or raw id
    s = str(link_or_id).strip()
    m = (
        re.search(r'id=([A-Za-z0-9_-]+)', s)
        or re.search(r'/d/([A-Za-z0-9_-]+)', s)
        or re.search(r'([A-Za-z0-9_-]{10,})$', s)
    )
    if not m:
        raise ValueError("Paste a valid Google Drive file link or id")
    fid = m.group(1)

    # Use project-local folder instead of /content
    Path(dest_folder).mkdir(parents=True, exist_ok=True)

    url = f"https://drive.google.com/uc?id={fid}"

    if not filename:
        filename = f"{fid}.pt"

    file_path = os.path.join(dest_folder, filename)

    print(f"Would download file to {file_path} ...")
    print("Example command:")
    print(f"  gdown {url!r} -O {file_path!r}")

# %% ../nbs/index.ipynb 10
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 13
#| eval: false
#| hide: true
import re
import os
import shutil
import subprocess
import zipfile

def download_and_extract_strip_top_folder(link_or_id: str, dest: str = "angle-models-weights"):
    """
    Download a zip file from Google Drive and extract its contents to `dest`,
    stripping the top-level folder if it exists.

    Example usage:
        download_and_extract_strip_top_folder("https://drive.google.com/file/d/1uI2GlE5JviTJccZxzLP5lQ6YiwjYTvVe/view?usp=sharing")
    """
    # Parse file ID
    m = re.search(r'id=([A-Za-z0-9_-]+)', link_or_id) or re.search(r'/d/([A-Za-z0-9_-]+)', link_or_id)
    if not m:
        raise ValueError("Paste a valid Google Drive file link")
    fid = m.group(1)

    url = f"https://drive.google.com/uc?id={fid}"

    if os.path.exists(dest):
        shutil.rmtree(dest)
    os.makedirs(dest, exist_ok=True)

    print("Downloading zip...")
    subprocess.run(["gdown", url, "-O", "weights.zip"], check=True)

    print("Extracting zip file and stripping top-level folder...")
    with zipfile.ZipFile("weights.zip", 'r') as zip_ref:
        all_paths = zip_ref.namelist()
        top_level_dirs = set([p.split('/')[0] for p in all_paths if p.strip() != ''])
        top_folder = list(top_level_dirs)[0] if len(top_level_dirs) == 1 else None

        for member in all_paths:
            if top_folder:
                rel_path = member[len(top_folder)+1:] if member.startswith(top_folder + '/') else member
            else:
                rel_path = member

            if rel_path == '':
                continue

            dest_path = os.path.join(dest, rel_path)
            if member.endswith('/'):
                os.makedirs(dest_path, exist_ok=True)
            else:
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                with zip_ref.open(member) as source, open(dest_path, 'wb') as target:
                    shutil.copyfileobj(source, target)

    print(f"Done — check the local '{dest}' folder.")

# %% ../nbs/index.ipynb 14
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 15
#| eval: false
#| hide: true
import re
import os
from pathlib import Path

def download_file_from_gdrive(link_or_id: str, dest_folder: str = "sam2_checkpoints", filename: str = None):
    """
    Helper to download a SAM2 checkpoint into the local 'sam2_checkpoints' folder.

    Example usage:
        download_file_from_gdrive(
            "https://drive.google.com/file/d/11L0iHZ1Ktcjhd0vqKI_3-PJ3DTv2UyOc/view?usp=sharing",
            dest_folder="sam2_checkpoints",
            filename="sam2_hiera_base_plus.pt"
        )
    """
    m = re.search(r'id=([A-Za-z0-9_-]+)', link_or_id) or re.search(r'/d/([A-Za-z0-9_-]+)', link_or_id)
    if not m:
        raise ValueError("Paste a valid Google Drive file link")
    fid = m.group(1)

    Path(dest_folder).mkdir(parents=True, exist_ok=True)

    url = f"https://drive.google.com/uc?id={fid}"

    if not filename:
        filename = f"{fid}.pt"

    file_path = os.path.join(dest_folder, filename)

    print(f"Would download file to {file_path} ...")
    print("Example command:")
    print(f"  gdown {url!r} -O {file_path!r}")

# %% ../nbs/index.ipynb 16
dataset = '/content/Dataset/AiBoardScannerDataSet.coco'

# %% ../nbs/index.ipynb 17
#| eval: false
#| hide: true
from PIL import Image
from pathlib import Path

# Show how to open the training metrics plot from the local 'weights' folder.
# This is documentation-only; not executed in nbdev docs.
img_path = Path("weights") / "metrics_plot.png"
print(f"Example usage to view the metrics plot:")
print(f"  Image.open({str(img_path)!r})")

# %% ../nbs/index.ipynb 18
preTrainedWeights = "/content/weights/checkpoint_best_regular.pth"

# %% ../nbs/index.ipynb 19
#| eval: false
import fitz
from PIL import Image
import os
import io

def load_or_convert_file(file_path: str, output_image_path: str = "converted_image.png") -> Image.Image | None:
    """
    Load an image from a file path, or convert the first page of a PDF to an image.

    Args:
        file_path (str): Path to the image or PDF file.
        output_image_path (str): Path to save the converted image if input is a PDF.

    Returns:
        PIL.Image.Image or None: Loaded image, or None if there was an error.
    """
    imgPath = None

    if not os.path.exists(file_path):
        print(f"Error: File not found at {file_path}")
    else:
        if file_path.lower().endswith('.pdf'):
            try:
                doc = fitz.open(file_path)
                page = doc.load_page(0)
                pix = page.get_pixmap()
                pix.save(output_image_path)
                imgPath = Image.open(output_image_path).convert("RGB")
                print("PDF converted to image and saved successfully.")
                doc.close()
            except Exception as e:
                print(f"Error converting PDF: {e}")
        else:
            try:
                imgPath = Image.open(file_path).convert("RGB")
                print("Image loaded successfully.")
            except Exception as e:
                print(f"Error opening image: {e}")

    return imgPath

# %% ../nbs/index.ipynb 20
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 21
#| eval: false
#| hide: true
import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRMedium
from rfdetr.util.coco_classes import COCO_CLASSES

# Example code for running RF-DETR on an image.
# This is for documentation only and is NOT executed during nbdev_docs.

# preTrainedWeights = "weights/checkpoint_best_regular.pth"
# imgPath = "path/to/your/test/image.png"

# model = RFDETRMedium(pretrain_weights=preTrainedWeights)
# model.optimize_for_inference()

# image = Image.open(imgPath).convert("RGB")
# detections = model.predict(image, threshold=0.60)

# text_scale = sv.calculate_optimal_text_scale(image.size)
# thickness = sv.calculate_optimal_line_thickness(image.size)
# color = sv.ColorPalette.from_hex([
#     "#FFFF00", "#FF9B00", "#FF66FF", "#3399FF", "#FF66B2", "#FF8080",
#     "#B266FF", "#9999FF", "#66FFFF", "#33FF99", "#66FF66", "#99FF00"
# ])

# bbox_annotator = sv.BoxAnnotator(color=color, thickness=thickness)
# label_annotator = sv.LabelAnnotator(color=color, text_color=sv.Color.BLACK, text_scale=text_scale)

# my_classes = [
#     "objects",
#     "Cloud",
#     "Diamond",
#     "Double Arrow",
#     "Pentagon",
#     "Racetrack",
#     "Star",
#     "Sticky Notes",
#     "Triangle",
#     "arrow",
#     "arrow_head",
#     "circle",
#     "dashed-arrow",
#     "dotted-arrow",
#     "rectangle",
#     "rounded rectangle",
#     "solid-arrow"
# ]
# detection_labels = [
#     f"{my_classes[class_id]} {conf:.2f}"
#     for class_id, conf in zip(detections.class_id, detections.confidence)
# ]

# detections_list=[]
# for bbox, class_id, conf in zip(detections.xyxy, detections.class_id, detections.confidence):
#     x_min, y_min, x_max, y_max = bbox
#     class_name = my_classes[class_id]
#     detection_tuple = (class_name, (float(x_min), float(y_min)), (float(x_max), float(y_max)))
#     detections_list.append(detection_tuple)

# print("detections = [")
# for d in detections_list:
#     print(f"    {d},")
# print("]")

# annotated_image = image.copy()
# annotated_image = bbox_annotator.annotate(annotated_image, detections)
# annotated_image = label_annotator.annotate(annotated_image, detections, detection_labels)

# sv.plot_image(annotated_image)

# %% ../nbs/index.ipynb 22
#| eval: false
#| hide: true
# Example: convert RF-DETR detections into a JSON-friendly structure.
# This cell is for documentation only and is NOT executed during nbdev_docs.

detected_objects = []
for class_id, conf, bbox in zip(detections.class_id, detections.confidence, detections.xyxy):
    x1, y1, x2, y2 = bbox.tolist()
    detected_objects.append({
        "class": my_classes[class_id],
        "confidence": float(conf),
        "bbox": {
            "x1": float(x1),
            "y1": float(y1),
            "x2": float(x2),
            "y2": float(y2),
        },
    })

import json
print(json.dumps(detected_objects, indent=2))

# %% ../nbs/index.ipynb 24
#| eval: false
#| hide: true
import cv2
import numpy as np
from paddleocr import PaddleOCR
import os

# Example: run PaddleOCR on a preprocessed image.
# This cell is for documentation only and is NOT executed during nbdev_docs.

# if file_path.lower().endswith('.pdf'):
#     image_for_ocr_path = output_image_path
# else:
#     image_for_ocr_path = file_path
#
# img = cv2.imread(image_for_ocr_path)
#
# kernel = np.array([[0, -1, 0],
#                    [-1, 3, -1],
#                    [0, -1, 0]])
#
# sharpened = cv2.filter2D(img, -1, kernel)
#
# alpha = 0.5
# sharpened = cv2.addWeighted(img, 1 - alpha, sharpened, alpha, 0)
#
# ocr = PaddleOCR(
#     use_doc_orientation_classify=False,
#     use_doc_unwarping=False,
#     use_textline_orientation=True,
#     lang='en',
# )
#
# result = ocr.predict(sharpened)
#
# out_dir = "OCRoutput"
# os.makedirs(out_dir, exist_ok=True)
#
# base_name = os.path.splitext(os.path.basename(file_path))[0]
#
# img_output_path = os.path.join(out_dir, f"{base_name}_ocr_res_img.png")
# json_output_path = os.path.join(out_dir, f"{base_name}_ocr_res.json")
#
# for i, res in enumerate(result):
#     res.print()
#     res.save_to_img(img_output_path)
#     res.save_to_json(json_output_path)
#     break

# %% ../nbs/index.ipynb 25
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 26
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 27
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 28
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 29
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 30
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 31
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 32
#| eval: false
import json
import math
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import os

STANDARD_ANGLES = [0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0, 360.0]
SNAP_TOLERANCE = 10.0

def _flatten_poly(poly):
    arr = np.array(poly, dtype=float)
    if arr.ndim == 1:
        arr = arr.reshape(-1, 2)
    return arr

def _box_area(b):
    return max(0, b[2] - b[0]) * max(0, b[3] - b[1])

def _intersection_area(a, b):
    ix1, iy1 = max(a[0], b[0]), max(a[1], b[1])
    ix2, iy2 = min(a[2], b[2]), min(a[3], b[3])
    return max(0, ix2 - ix1) * max(0, iy2 - iy1)

def calculate_rotation_from_polygon(polygon):
    pts = _flatten_poly(polygon)
    n = pts.shape[0]
    if n < 2:
        return 0.0
    if n == 2:
        dx = pts[1, 0] - pts[0, 0]
        dy = pts[1, 1] - pts[0, 1]
        return (math.degrees(math.atan2(-dy, dx)) + 360) % 360
    centered = pts - pts.mean(axis=0)
    if np.allclose(centered, 0, atol=1e-6):
        return 0.0
    _, _, Vt = np.linalg.svd(centered, full_matrices=False)
    vx, vy = Vt[0]
    pca_angle = (math.degrees(math.atan2(-vy, vx)) + 360) % 360
    x = pts[:, 0]
    y = pts[:, 1]
    if len(np.unique(x)) > 1:
        a, b = np.polyfit(x, y, 1)
        lf_angle = (math.degrees(math.atan2(-a, 1)) + 360) % 360
    else:
        lf_angle = pca_angle
    pca_var = np.var(centered @ np.array([vx, vy]))
    lf_dir = np.array([1, a]) / np.linalg.norm([1, a]) if len(np.unique(x)) > 1 else np.array([1, 0])
    lf_var = np.var(centered @ lf_dir)
    best = pca_angle if pca_var >= lf_var else lf_angle
    return round(best, 2)

def snap_to_standard_angle(angle, tolerance=SNAP_TOLERANCE):
    angle = angle % 360
    best = min(STANDARD_ANGLES, key=lambda s: abs(angle - s))
    return int(best) if abs(best - angle) <= tolerance else round(angle, 1)

def get_text_rotation_for_shape(shape_bbox, ocr_data, threshold=0.25):
    sx1, sy1, sx2, sy2 = shape_bbox
    weighted_vector = np.array([0.0, 0.0])
    total_weight = 0
    matched = False
    for item in ocr_data:
        tx1, ty1, tx2, ty2 = item["bbox"]
        text_area = _box_area(item["bbox"])
        if text_area == 0:
            continue
        inter = _intersection_area(shape_bbox, item["bbox"])
        overlap = inter / text_area
        if overlap < threshold:
            continue
        matched = True
        conf = float(item.get("confidence", 1.0))
        ang = calculate_rotation_from_polygon(item.get("polygon", [(tx1, ty1), (tx2, ty2)]))
        rad = math.radians(ang)
        ux, uy = math.cos(rad), math.sin(rad)
        w = overlap * conf * max(1, text_area)
        weighted_vector += w * np.array([ux, uy])
        total_weight += w
    if not matched or total_weight == 0:
        return None
    mean_vec = weighted_vector / total_weight
    mean_angle = (math.degrees(math.atan2(mean_vec[1], mean_vec[0])) + 360) % 360
    mean_angle = round(mean_angle, 2)
    snapped = snap_to_standard_angle(mean_angle)
    return {"raw_rotation": mean_angle, "shape_rotation": snapped}

def process_paddleocr_json(json_path, min_confidence=0.4):
    processed = []
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            ocr = json.load(f)
    except:
        return []
    texts = ocr.get("rec_texts", [])
    scores = ocr.get("rec_scores", [])
    polys = ocr.get("rec_polys", [])
    orientation = ocr.get("textline_orientation_angles", [])
    N = min(len(texts), len(scores), len(polys))
    for i in range(N):
        if scores[i] < min_confidence:
            continue
        poly = _flatten_poly(polys[i])
        x1, y1 = np.min(poly, axis=0)
        x2, y2 = np.max(poly, axis=0)
        raw_rot = calculate_rotation_from_polygon(poly)
        snapped = snap_to_standard_angle(raw_rot)
        processed.append({
            "text": texts[i],
            "confidence": float(scores[i]),
            "bbox": (int(x1), int(y1), int(x2), int(y2)),
            "polygon": polys[i],
            "paddleocr_orientation": orientation[i] if i < len(orientation) else None,
            "polygon_rotation": raw_rot,
            "shape_rotation": snapped
        })
    return processed

def plot_ocr_detections(data):
    if not data:
        print("No data.")
        return
    fig, ax = plt.subplots(1, figsize=(14, 12))
    ax.set_facecolor("black")
    for item in data:
        x1, y1, x2, y2 = item["bbox"]
        raw = item["polygon_rotation"]
        snap = item["shape_rotation"]
        ax.add_patch(
            patches.Rectangle((x1, y1), x2 - x1, y2 - y1, ec="r", fc="none", lw=1)
        )
        ax.text(x1, y1 - 5, f"{item['text']} ({raw}° → {snap}°)",
                fontsize=6, color="white", backgroundcolor="red")
    xs = [d["bbox"][0] for d in data] + [d["bbox"][2] for d in data]
    ys = [d["bbox"][1] for d in data] + [d["bbox"][3] for d in data]
    ax.set_xlim(min(xs) - 10, max(xs) + 10)
    ax.set_ylim(max(ys) + 10, min(ys) - 10)
    ax.invert_yaxis()
    ax.set_aspect("equal")
    plt.axis("off")
    plt.show()


#| hide
# Example usage (for documentation/testing purposes)
# base_name = os.path.splitext(os.path.basename(file_path))[0]
# json_output_path = f"OCRoutput/{base_name}_ocr_res.json"
# if os.path.exists(json_output_path):
#     final_processed_data = process_paddleocr_json(json_output_path)
#     if final_processed_data:
#         print("Processed OCR data (full 360° rotation):")
#         print("-" * 70)
#         for item in final_processed_data:
#             print(f"{item['text'][:20]:<20} | Raw: {item['polygon_rotation']:6.1f}° → Rounded: {item['shape_rotation']}°")
#         print("-" * 70)
#         plot_ocr_detections(final_processed_data)
# else:
#     print(f"JSON not found: {json_output_path}")

# %% ../nbs/index.ipynb 33
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 34
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 35
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 36
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 37
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 38
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 39
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 40
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 41
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 42
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 43
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 44
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 45
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 46
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 47
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 48
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 49
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 50
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 51
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 52
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 53
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 54
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 55
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 56
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 57
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 58
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 59
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 60
#| eval: false
#| echo: false
#| output: asis#| hide
#| export
#| eval: false
#| echo: false
#| output: asis

# %% ../nbs/index.ipynb 61
#| eval: false
#| echo: true
import cv2
import matplotlib.pyplot as plt
from skimage.morphology import skeletonize
from collections import deque
import itertools
import math

# ==============================================================================
# ---------- HELPER FUNCTIONS  ----------
# ==============================================================================

def show_image(title, img, cmap=None):
    plt.figure(figsize=(4, 4))
    if len(img.shape) == 2: plt.imshow(img, cmap=cmap or "gray")
    else: plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title(title); plt.axis("off"); plt.show()

def crop_image_region(image, top_left, bottom_right):
    x1, y1 = map(int, map(round, top_left))
    x2, y2 = map(int, map(round, bottom_right))
    return image[y1:y2, x1:x2]

def convert_to_binary_mask(cropped_img):
    if cropped_img.size == 0:
        return np.array([], dtype=np.uint8)
    gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (3,3), 0)

    edges = cv2.Canny(blurred, threshold1=30, threshold2=100)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))
    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=1)

    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    if np.mean(thresh) > 127:
        thresh = cv2.bitwise_not(thresh)
    combined_mask = cv2.bitwise_or(closed, thresh)
    return combined_mask


def connect_dotted_simple_morph(binary_mask, kernel_size=5, iterations=2):

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
    closed_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel, iterations=1)
    connected_mask = cv2.dilate(closed_mask, kernel, iterations=iterations)
    return connected_mask


def find_farthest_points_in_contour(contour):
    max_dist_sq, best_pair = -1, None
    points = contour.squeeze(axis=1)
    if len(points) < 2: return None
    for p1, p2 in itertools.combinations(points, 2):
        dist_sq = (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2
        if dist_sq > max_dist_sq: max_dist_sq, best_pair = dist_sq, (tuple(p1), tuple(p2))
    return best_pair

def interpolate_dashed_arrows(binary_mask, max_gap_ratio=0.5, thickness=2):
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = [c for c in contours if cv2.contourArea(c) > 3]

    if len(contours) < 2:
        return binary_mask

    dash_endpoints = []
    for cnt in contours:
        end_pts = find_farthest_points_in_contour(cnt)
        if end_pts:
            dash_endpoints.append(list(end_pts))

    dash_lengths = [math.dist(ep[0], ep[1]) for ep in dash_endpoints if ep]
    if not dash_lengths:
        return binary_mask
    max_connect_dist = np.median(dash_lengths) * (1 + max_gap_ratio)

    output_mask = binary_mask.copy()

    for i, eps1 in enumerate(dash_endpoints):
        for ep1 in eps1:
            min_dist = float('inf')
            best_ep = None
            for j, eps2 in enumerate(dash_endpoints):
                if i == j:
                    continue
                for ep2 in eps2:
                    dist = math.dist(ep1, ep2)
                    if dist < min_dist:
                        min_dist = dist
                        best_ep = ep2
            if best_ep and min_dist < max_connect_dist:
                cv2.line(output_mask, ep1, best_ep, 255, thickness=thickness)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 5))
    output_mask = cv2.dilate(output_mask, kernel, iterations=2)

    return output_mask


def extract_skeleton(binary_mask):
    return skeletonize(binary_mask > 0).astype(np.uint8) * 255


def get_skeleton_graph_nodes(skeleton):
    endpoints, junctions = [], []
    h, w = skeleton.shape
    padded_skeleton = np.pad(skeleton, 1, 'constant')
    for y_pad in range(1, h + 1):
        for x_pad in range(1, w + 1):
            if padded_skeleton[y_pad, x_pad] > 0:
                num_neighbors = np.sum(padded_skeleton[y_pad-1:y_pad+2, x_pad-1:x_pad+2] > 0) - 1
                if num_neighbors == 1: endpoints.append((x_pad - 1, y_pad - 1))
                elif num_neighbors > 2: junctions.append((x_pad - 1, y_pad - 1))
    return endpoints, junctions


def find_farthest_points_euclidean(binary_mask):
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return []
    all_points = np.vstack([cnt.squeeze(axis=1) for cnt in contours if cnt.ndim > 2])
    if len(all_points) < 2: return []
    return find_farthest_points_in_contour(np.array(all_points).reshape(-1, 1, 2))


def check_bbox_intersection(box1_tl, box1_br, box2_tl, box2_br):
    return not (box1_br[0] < box2_tl[0] or box1_tl[0] > box2_br[0] or box1_br[1] < box2_tl[1] or box1_tl[1] > box2_br[1])


def simple_bridge_by_interpolation(binary_mask, ocr_bbox_local, thickness=1):

    h, w = binary_mask.shape[:2]
    x1, y1, x2, y2 = map(int, ocr_bbox_local)
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)
    if x2 <= x1 or y2 <= y1:
        return binary_mask.copy()

    mask_no_text = binary_mask.copy()
    mask_no_text[y1:y2, x1:x2] = 0

    pts = np.column_stack(np.where(mask_no_text > 0))
    if pts.size == 0:
        return binary_mask.copy()

    left_pts = pts[pts[:, 1] < x1]
    right_pts = pts[pts[:, 1] > x2]

    if left_pts.size and right_pts.size:
        left_idx = np.argmax(left_pts[:, 1])
        right_idx = np.argmin(right_pts[:, 1])
        left_p = (int(left_pts[left_idx, 1]), int(left_pts[left_idx, 0]))
        right_p = (int(right_pts[right_idx, 1]), int(right_pts[right_idx, 0]))
        p1, p2 = left_p, right_p
    else:
        top_pts = pts[pts[:, 0] < y1]
        bottom_pts = pts[pts[:, 0] > y2]
        if top_pts.size and bottom_pts.size:
            top_idx = np.argmax(top_pts[:, 0])
            bot_idx = np.argmin(bottom_pts[:, 0])
            top_p = (int(top_pts[top_idx, 1]), int(top_pts[top_idx, 0]))
            bot_p = (int(bottom_pts[bot_idx, 1]), int(bottom_pts[bot_idx, 0]))
            p1, p2 = top_p, bot_p
        else:
            coords = np.column_stack((pts[:,1], pts[:,0]))
            max_d = -1; best = None
            for i in range(len(coords)):
                for j in range(i+1, len(coords)):
                    d = (coords[i,0]-coords[j,0])**2 + (coords[i,1]-coords[j,1])**2
                    if d > max_d:
                        max_d = d
                        best = (tuple(coords[i].tolist()), tuple(coords[j].tolist()))
            if not best:
                return binary_mask.copy()
            p1, p2 = best

    out = mask_no_text.copy()
    dist = int(math.hypot(p2[0]-p1[0], p2[1]-p1[1]))
    if dist < 2:
        cv2.line(out, p1, p2, 255, thickness=max(1, thickness))
        return out

    n = max(dist, 2)
    xs = np.linspace(p1[0], p2[0], n)
    ys = np.linspace(p1[1], p2[1], n)
    for x, y in zip(xs, ys):
        cx, cy = int(round(x)), int(round(y))
        x0, x1_ = max(0, cx - thickness), min(w, cx + thickness + 1)
        y0, y1_ = max(0, cy - thickness), min(h, cy + thickness + 1)
        out[y0:y1_, x0:x1_] = 255

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    out = cv2.dilate(out, kernel, iterations=1)
    return out

def bbox_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    if interArea == 0:
        return 0.0
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    return interArea / float(boxAArea + boxBArea - interArea)

def pixel_level_ocr_interference(binary_arrow_mask, ocr_bbox, crop_origin, coverage_threshold=0.05):
    crop_x1, crop_y1 = crop_origin
    h, w = binary_arrow_mask.shape

    ox1, oy1, ox2, oy2 = ocr_bbox
    local_x1 = max(0, int(ox1 - crop_x1))
    local_y1 = max(0, int(oy1 - crop_y1))
    local_x2 = min(w, int(ox2 - crop_x1))
    local_y2 = min(h, int(oy2 - crop_y1))

    if local_x1 >= local_x2 or local_y1 >= local_y2:
        return False

    arrow_pixels = np.count_nonzero(binary_arrow_mask)
    if arrow_pixels == 0:
        return False

    overlap_region = binary_arrow_mask[local_y1:local_y2, local_x1:local_x2]
    overlap_count = np.count_nonzero(overlap_region)
    coverage_ratio = overlap_count / arrow_pixels
    return coverage_ratio > coverage_threshold

def mask_overlapping_ocr_regions(binary_mask, ocr_intrusions, crop_origin):
    mask = binary_mask.copy()
    crop_x1, crop_y1 = crop_origin
    h, w = mask.shape

    for ocr in ocr_intrusions:
        ox1, oy1, ox2, oy2 = ocr['bbox']
        local_x1 = max(0, int(ox1 - crop_x1))
        local_y1 = max(0, int(oy1 - crop_y1))
        local_x2 = min(w, int(ox2 - crop_x1))
        local_y2 = min(h, int(oy2 - crop_y1))
        if local_x1 < local_x2 and local_y1 < local_y2:
            mask[local_y1:local_y2, local_x1:local_x2] = 0
    return mask

def match_arrowhead_to_endpoint(arrowhead_bboxes, endpoints, crop_origin):

    if not arrowhead_bboxes:
        return None

    crop_x, crop_y = crop_origin
    arrowhead_centers = []
    for tl, br in arrowhead_bboxes:
        cx = (tl[0] + br[0]) / 2 - crop_x
        cy = (tl[1] + br[1]) / 2 - crop_y
        arrowhead_centers.append((cx, cy))

    for center in arrowhead_centers:
        for ep in endpoints:

            if abs(ep[0] - center[0]) < 15 and abs(ep[1] - center[1]) < 12:
                return ep
    return None

# =================================================================================
# ---------- FOR INTERSECTING ARROWS ----------
# =================================================================================

def bfs_get_path(skeleton, start_xy, end_xy):


    h, w = skeleton.shape
    start_rc, end_rc = (start_xy[1], start_xy[0]), (end_xy[1], end_xy[0])

    if not (0 <= start_rc[0] < h and 0 <= start_rc[1] < w and skeleton[start_rc] > 0): return None
    if not (0 <= end_rc[0] < h and 0 <= end_rc[1] < w and skeleton[end_rc] > 0): return None

    q = deque([(start_rc, [start_xy])])
    visited = {start_rc}

    while q:
        (r, c), path = q.popleft()

        if (r, c) == end_rc:
            return path

        for dr, dc in itertools.product([-1, 0, 1], repeat=2):
            if dr == 0 and dc == 0: continue
            nr, nc = r + dr, c + dc

            if 0 <= nr < h and 0 <= nc < w and skeleton[nr, nc] > 0 and (nr, nc) not in visited:
                visited.add((nr, nc))
                new_path = path + [(nc, nr)]
                q.append(((nr, nc), new_path))

    return None

def find_best_arrow_by_straightness(binary_mask, min_path_length=20):

    skeleton = extract_skeleton(binary_mask)
    if np.count_nonzero(skeleton) < min_path_length:
        return None

    endpoints, _ = get_skeleton_graph_nodes(skeleton)

    if len(endpoints) < 2:
        return find_farthest_points_euclidean(binary_mask)

    candidate_paths = []
    for p1, p2 in itertools.combinations(endpoints, 2):
        path = bfs_get_path(skeleton, p1, p2)

        if path and len(path) >= min_path_length:
            path_length = len(path)
            euclidean_dist = math.dist(p1, p2)

            straightness = euclidean_dist / path_length

            score = path_length * straightness

            candidate_paths.append({
                "endpoints": [p1, p2],
                "score": score,
                "length": path_length
            })

    if not candidate_paths:
        return None

    best_path = max(candidate_paths, key=lambda x: x['score'])
    return best_path['endpoints']


## ==============================================================================
# ----------   MAIN PIPELINE FUNCTION  ----------
# ==============================================================================

def skeleton_has_cycle(skeleton):
    h, w = skeleton.shape
    visited = np.zeros((h, w), dtype=bool)

    def neighbors(r, c):
        for dr in [-1, 0, 1]:
            for dc in [-1, 0, 1]:
                if dr == 0 and dc == 0:
                    continue
                nr, nc = r + dr, c + dc
                if 0 <= nr < h and 0 <= nc < w and skeleton[nr, nc] > 0:
                    yield nr, nc

    def dfs(r, c, pr, pc):
        visited[r, c] = True
        for nr, nc in neighbors(r, c):
            if not visited[nr, nc]:
                if dfs(nr, nc, r, c):
                    return True
            elif (nr, nc) != (pr, pc):
                return True
        return False

    for y in range(h):
        for x in range(w):
            if skeleton[y, x] > 0 and not visited[y, x]:
                if dfs(y, x, -1, -1):
                    return True
    return False


def find_farthest_geodesic_endpoints(skeleton):
    endpoints, _ = get_skeleton_graph_nodes(skeleton)
    if len(endpoints) < 2: return None
    max_length, best_pair = -1, None
    for p1, p2 in itertools.combinations(endpoints, 2):
        path = bfs_get_path(skeleton, p1, p2)
        if path and len(path) > max_length:
            max_length, best_pair = len(path), (p1, p2)
    return best_pair


def dilate_ocr_regions(binary_mask, ocr_intrusions, crop_origin,
                       vert_expand=1, horiz_expand=1,
                       vert_iterations=1, horiz_iterations=1,
                       erosion_kernel_size=3, erosion_iterations=1):
    mask = np.zeros_like(binary_mask)
    crop_x1, crop_y1 = crop_origin
    h, w = mask.shape
    for ocr in ocr_intrusions:
        ox1, oy1, ox2, oy2 = ocr['bbox']
        local_x1 = max(0, int(ox1 - crop_x1))
        local_y1 = max(0, int(oy1 - crop_y1))
        local_x2 = min(w, int(ox2 - crop_x1))
        local_y2 = min(h, int(oy2 - crop_y1))
        if local_x1 < local_x2 and local_y1 < local_y2:
            mask[local_y1:local_y2, local_x1:local_x2] = 255

    vert_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vert_expand))
    horiz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (horiz_expand, 1))

    mask = cv2.dilate(mask, vert_kernel, iterations=vert_iterations)
    mask = cv2.dilate(mask, horiz_kernel, iterations=horiz_iterations)

    combined = cv2.bitwise_or(binary_mask, mask)

    erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,
                                               (erosion_kernel_size, erosion_kernel_size))
    combined = cv2.erode(combined, erosion_kernel, iterations=erosion_iterations)

    return combined


def find_skeleton_points_on_bbox(skeleton, bbox_tl, bbox_br, margin=2):
    points = np.column_stack(np.where(skeleton > 0))
    x_min, y_min = bbox_tl
    x_max, y_max = bbox_br
    candidate_points = []
    for y, x in points:
        near_left = abs(x - x_min) <= margin and y_min <= y <= y_max
        near_right = abs(x - x_max) <= margin and y_min <= y <= y_max
        near_top = abs(y - y_min) <= margin and x_min <= x <= x_max
        near_bottom = abs(y - y_max) <= margin and x_min <= x <= x_max
        if near_left or near_right or near_top or near_bottom:
            candidate_points.append((x, y))
    return candidate_points


def debug_stage(image, detections, ocr_results):
    vectors = []
    arrow_labels = ["dashed-arrow", "dotted-arrow", "solid-arrow"]
    non_arrow_labels = ["Diamond"]

    arrows = [d for d in detections if d[0] in arrow_labels]
    interfering_objects = [d for d in detections if d[0] in non_arrow_labels]

    # ... keep the rest of debug_stage body exactly as you have it ...

    return vectors


# Example usage (for documentation only; not executed by nbdev_docs):
# image = cv2.imread(file_path)
# detections = detections_list
# vectors = debug_stage(image, detections, final_processed_data)
# print("\n=== Results from Full Pipeline ===")
# for v in vectors:
#     print(f"Arrow: Tail={v['tail']} → Head={v['head']} -> Label={v['label']}")

# %% ../nbs/index.ipynb 62
#| eval: false
#| echo: true
from scipy.interpolate import splprep, splev

def show_image(title, img, cmap=None):
    plt.figure(figsize=(12, 12))
    if len(img.shape) == 2:
        plt.imshow(img, cmap=cmap or "gray")
    else:
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title(title, fontsize=16)
    plt.axis("off")
    plt.show()

def point_to_bbox_distance(point, top_left, bottom_right):
    px, py = point
    x1, y1 = top_left
    x2, y2 = bottom_right

    if x1 <= px <= x2 and y1 <= py <= y2:
        return 0.0

    dx = max(x1 - px, 0, px - x2)
    dy = max(y1 - py, 0, py - y2)
    return math.sqrt(dx*dx + dy*dy)

def nearest_point_on_bbox(point, top_left, bottom_right):
    px, py = point
    x1, y1 = top_left
    x2, y2 = bottom_right
    cx = min(max(px, x1), x2)
    cy = min(max(py, y1), y2)
    return (int(round(cx)), int(round(cy)))

def find_nearest_shape_bbox(point, shapes, max_distance=None):
    if not shapes:
        return None, None, None

    min_dist = float('inf')
    nearest_shape = None
    nearest_pt = None

    for shape in shapes:
        label, top_left, bottom_right = shape
        pt_on_bbox = nearest_point_on_bbox(point, top_left, bottom_right)
        dist = math.hypot(pt_on_bbox[0] - point[0], pt_on_bbox[1] - point[1])

        if dist < min_dist:
            min_dist = dist
            nearest_shape = shape
            nearest_pt = pt_on_bbox

    if max_distance is not None and min_dist > max_distance:
        return None, None, None

    return nearest_shape, nearest_pt, min_dist

def draw_connections_on_image(image, connections):
    vis_image = image.copy()

    ARROW_COLOR = (0, 255, 0)        # green
    HEAD_COLOR = (255, 0, 0)         # red
    TAIL_COLOR = (0, 255, 255)       # cyan
    CONNECTION_COLOR = (255, 0, 255) # magenta

    for conn in connections:
        head = conn["head"]
        tail = conn["tail"]

        cv2.arrowedLine(vis_image, tail, head, ARROW_COLOR, 2, tipLength=0.04)

        cv2.circle(vis_image, head, 8, HEAD_COLOR, -1)
        cv2.circle(vis_image, tail, 8, TAIL_COLOR, -1)

        if conn.get("head_connected_to") is not None:
            head_pt = conn.get("head_connection_point")
            if head_pt is None:
                _, tl, br = conn["head_connected_to"]
                head_pt = (int((tl[0] + br[0]) / 2), int((tl[1] + br[1]) / 2))
            cv2.line(vis_image, head, head_pt, CONNECTION_COLOR, 2, cv2.LINE_AA)
            cv2.circle(vis_image, head_pt, 5, CONNECTION_COLOR, -1)

        if conn.get("tail_connected_to") is not None:
            tail_pt = conn.get("tail_connection_point")
            if tail_pt is None:
                _, tl, br = conn["tail_connected_to"]
                tail_pt = (int((tl[0] + br[0]) / 2), int((tl[1] + br[1]) / 2))
            cv2.line(vis_image, tail, tail_pt, CONNECTION_COLOR, 2, cv2.LINE_AA)
            cv2.circle(vis_image, tail_pt, 5, CONNECTION_COLOR, -1)

    return vis_image

def establish_connections(vectors, all_detections):
    labels_to_exclude = ["dashed-arrow", "dotted-arrow", "solid-arrow", "arrow_head"]
    shape_detections = [d for d in all_detections if d[0] not in labels_to_exclude]

    connections = []

    for vector in vectors:
        head_point = vector["head"]
        tail_point = vector["tail"]

        MAX_CONNECTION_DISTANCE = 44

        head_shape, head_pt_on_bbox, head_dist = find_nearest_shape_bbox(
            head_point, shape_detections, max_distance=MAX_CONNECTION_DISTANCE
        )
        tail_shape, tail_pt_on_bbox, tail_dist = find_nearest_shape_bbox(
            tail_point, shape_detections, max_distance=MAX_CONNECTION_DISTANCE
        )

        connections.append({
            "head": head_point,
            "tail": tail_point,
            "head_connected_to": head_shape,
            "head_connection_point": head_pt_on_bbox,
            "head_connection_dist": head_dist,
            "tail_connected_to": tail_shape,
            "tail_connection_point": tail_pt_on_bbox,
            "tail_connection_dist": tail_dist,
            "original_label": vector.get("label", "")
        })

    return connections

# Example usage (docs only; not executed by nbdev_docs):
# arrow_connections = establish_connections(vectors, detections)
# print("\n--- ARROW CONNECTION RESULTS ---")
# for i, conn in enumerate(arrow_connections):
#     head_label = conn['head_connected_to'][0] if conn['head_connected_to'] else 'None'
#     tail_label = conn['tail_connected_to'][0] if conn['tail_connected_to'] else 'None'
#     print(
#         f"Arrow {i} ({conn['original_label']}):\n"
#         f"  - TAIL at {conn['tail']} connects to -> {tail_label}\n"
#         f"  - HEAD at {conn['head']} connects to -> {head_label}\n"
#     )
#
# visualization_image = draw_connections_on_image(image, arrow_connections)
# show_image("Final Arrow Connections", visualization_image)

# %% ../nbs/index.ipynb 64
#| eval: false
def get_text_rotation_for_shape(shape_bbox, ocr_data, threshold=0.5):

    sx1, sy1, sx2, sy2 = shape_bbox
    shape_area = (sx2 - sx1) * (sy2 - sy1)

    best_overlap = 0
    best_rotation = None

    for ocr_item in ocr_data:
        tx1, ty1, tx2, ty2 = ocr_item['bbox']

        ix1 = max(sx1, tx1)
        iy1 = max(sy1, ty1)
        ix2 = min(sx2, tx2)
        iy2 = min(sy2, ty2)

        if ix1 < ix2 and iy1 < iy2:
            intersection = (ix2 - ix1) * (iy2 - iy1)
            text_area = (tx2 - tx1) * (ty2 - ty1)

            overlap_ratio = intersection / text_area if text_area > 0 else 0

            if overlap_ratio > threshold and overlap_ratio > best_overlap:
                best_overlap = overlap_ratio
                best_rotation = ocr_item.get('polygon_rotation',
                                            ocr_item.get('paddleocr_orientation', 0))

    return best_rotation

# %% ../nbs/index.ipynb 65
#| eval: false
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import os
import matplotlib.pyplot as plt
import numpy as np
import cv2
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
def check_overlap(box1, box2):
    x1_a, y1_a, x2_a, y2_a = box1
    x1_b, y1_b, x2_b, y2_b = box2
    if x2_a < x1_b or x2_b < x1_a:
        return False
    if y2_a < y1_b or y2_b < y1_a:
        return False
    return True
detections = detections_list
include_labels = set([
    "Pentagon", "Racetrack",
    "Triangle", "arrow", "rectangle"
])
filtered_detections = [det for det in detections if det[0] in include_labels]
boxes = [[float(x1), float(y1), float(x2), float(y2)] for _, (x1, y1), (x2, y2) in filtered_detections]
os.chdir("/content/sam2/sam2/configs/sam2")
model_cfg = "sam2_hiera_b+.yaml"
sam2_checkpoint = "/content/sam2/checkpoints/sam2_hiera_base_plus.pt"
sam2_model = build_sam2(model_cfg, sam2_checkpoint, device="cpu")
predictor = SAM2ImagePredictor(sam2_model)
image = Image.open(file_path).convert("RGB")
completable_shapes = {'circle', 'rounded rectangle', 'rectangle', 'Racetrack'}
occlusion_flags = []
for i in range(len(filtered_detections)):
    is_occluded = False
    label_i = filtered_detections[i][0]
    box_i = boxes[i]
    if label_i in completable_shapes:
        for j in range(len(filtered_detections)):
            if i == j: continue
            box_j = boxes[j]
            if check_overlap(box_i, box_j):
                is_occluded = True
                break
    occlusion_flags.append(is_occluded)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
angle_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])
])
model_configs = {
    'rectangle': {"path": "/content/angle-models-weights/final_rectangle.pth", "num_classes": 4},
    'Triangle': {"path": "/content/angle-models-weights/Triangle.pth", "num_classes": 4},
    'Racetrack': {"path": "/content/angle-models-weights/final_racetrack.pth", "num_classes": 4},
    'Pentagon': {"path": "/content/angle-models-weights/best_resnet18_pentagon.pth", "num_classes": 2},
    'arrow': {"path": "/content/angle-models-weights/best_resnet18_arrow.pth", "num_classes": 4},
}
angle_models = {}
for shape_name, config in model_configs.items():
    if os.path.exists(config["path"]):
        model = models.resnet18(pretrained=False)
        model.fc = nn.Linear(model.fc.in_features, config["num_classes"])
        model.load_state_dict(torch.load(config["path"], map_location=device))
        model = model.to(device)
        model.eval()
        angle_models[shape_name] = model
        print(f"Loaded angle model for {shape_name} from {config['path']}")
    else:
        print(f"Warning: Angle model for {shape_name} not found at {config['path']}")
triangle_aux_model_path = "/content/angle-models-weights/Triangle_Angle_90_270.pth"
if os.path.exists(triangle_aux_model_path):
    triangle_aux_model = models.resnet18(pretrained=False)
    triangle_aux_model.fc = nn.Linear(triangle_aux_model.fc.in_features, 2)
    triangle_aux_model.load_state_dict(torch.load(triangle_aux_model_path, map_location=device))
    triangle_aux_model = triangle_aux_model.to(device)
    triangle_aux_model.eval()
    angle_models["triangle_aux"] = triangle_aux_model
def predict_angle(pil_image, model):
    img_tensor = angle_transform(pil_image).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(img_tensor)
        _, pred = torch.max(outputs, 1)
    return pred.item()
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.set_image(image)
    masks, iou_predictions, _ = predictor.predict(box=boxes)
print(f"Processed {len(boxes)} bounding boxes.")
print(f"Received {len(masks)} sets of mask candidates.")

predicted_angles = []
for i in range(len(boxes)):
    label = filtered_detections[i][0]
    is_occluded = occlusion_flags[i]
    original_bbox = boxes[i]
    box_masks = masks[i]
    box_scores = iou_predictions[i]
    best_mask_index = torch.argmax(torch.from_numpy(box_scores))
    best_mask = box_masks[best_mask_index]
    mask_binary = (best_mask > 0).astype(np.uint8)
    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        largest_contour = max(contours, key=cv2.contourArea)
        processed_mask = np.zeros_like(mask_binary)
        if label in completable_shapes and is_occluded:
            print(f"Applying occlusion correction to '{label}' (Box {i}).")
            if label == 'circle':
                (x, y), radius = cv2.minEnclosingCircle(largest_contour)
                cv2.circle(processed_mask, (int(x), int(y)), int(radius), (1), thickness=cv2.FILLED)
            elif label == 'Racetrack':
                rect = cv2.minAreaRect(largest_contour)
                center, (width, height), angle = rect
                if width < height:
                    width, height = height, width
                    angle += 90
                radius = height / 2
                if width <= height:
                    cv2.circle(processed_mask, (int(center[0]), int(center[1])), int(radius), (1), thickness=cv2.FILLED)
                else:
                    rect_center = np.array(center)
                    angle_rad = np.deg2rad(angle)
                    half_rect_len = width / 2 - radius
                    dx = np.cos(angle_rad) * half_rect_len
                    dy = np.sin(angle_rad) * half_rect_len
                    p1 = rect_center + np.array([dx, dy])
                    p2 = rect_center - np.array([dx, dy])
                    cv2.circle(processed_mask, (int(p1[0]), int(p1[1])), int(radius), (1), thickness=cv2.FILLED)
                    cv2.circle(processed_mask, (int(p2[0]), int(p2[1])), int(radius), (1), thickness=cv2.FILLED)
                    perp_dx = -np.sin(angle_rad) * radius
                    perp_dy = np.cos(angle_rad) * radius
                    corner1 = p1 + np.array([perp_dx, perp_dy])
                    corner2 = p1 - np.array([perp_dx, perp_dy])
                    corner3 = p2 - np.array([perp_dx, perp_dy])
                    corner4 = p2 + np.array([perp_dx, perp_dy])
                    rect_contour = np.array([corner1, corner2, corner3, corner4], dtype=np.intp)
                    cv2.drawContours(processed_mask, [rect_contour], 0, (1), thickness=cv2.FILLED)
            else:
                hull = cv2.convexHull(largest_contour)
                cv2.drawContours(processed_mask, [hull], -1, (1), thickness=cv2.FILLED)
        else:
            cv2.drawContours(processed_mask, [largest_contour], -1, (1), thickness=cv2.FILLED)
        mask_binary = processed_mask

    rows = np.any(mask_binary, axis=1)
    cols = np.any(mask_binary, axis=0)
    if not rows.any() or not cols.any():
        print(f"Mask for box {i} is empty after processing, skipping.")
        continue
    rmin, rmax = np.where(rows)[0][[0, -1]]
    cmin, cmax = np.where(cols)[0][[0, -1]]
    padding_percentage = 0.1
    height_pad = rmax - rmin
    width_pad = cmax - cmin
    padding_y = int(height_pad * padding_percentage)
    padding_x = int(width_pad * padding_percentage)
    padded_rmin = rmin - padding_y
    padded_rmax = rmax + padding_y
    padded_cmin = cmin - padding_x
    padded_cmax = cmax + padding_x
    full_mask_height, full_mask_width = mask_binary.shape
    padded_rmin = max(0, padded_rmin)
    padded_rmax = min(full_mask_height - 1, padded_rmax)
    padded_cmin = max(0, padded_cmin)
    padded_cmax = min(full_mask_width - 1, padded_cmax)
    padded_crop = mask_binary[padded_rmin:padded_rmax+1, padded_cmin:padded_cmax+1]
    final_image_rgb = np.zeros((padded_crop.shape[0], padded_crop.shape[1], 3), dtype=np.uint8)
    random_color = np.random.randint(0, 256, size=3)
    for c in range(3):
        final_image_rgb[:, :, c][padded_crop == 1] = random_color[c]
    cropped_pil_image = Image.fromarray(final_image_rgb)

    predicted_angle_info = "N/A"
    angle_source = "none"


    x1, y1, x2, y2 = original_bbox
    shape_bbox = (int(x1), int(y1), int(x2), int(y2))

    if 'final_processed_data' in dir() and final_processed_data:
        text_rotation = get_text_rotation_for_shape(shape_bbox, final_processed_data)
        if text_rotation is not None:
            normalized_rotation = text_rotation % 360
            predicted_angle_info = f"{normalized_rotation:.1f} "
            angle_source = "ocr_text"
            print(f"Using OCR text rotation for '{label}' (Box {i}): {predicted_angle_info}")


    if angle_source == "none" and label in angle_models:
        angle_model = angle_models[label]
        num_classes = model_configs[label]["num_classes"]
        angle_class = predict_angle(cropped_pil_image, angle_model)
        angle_source = "resnet_model"

        if label == 'Pentagon':
            if angle_class == 0:
                predicted_angle_info = "0 "
            elif angle_class == 1:
                predicted_angle_info = "180 "
            else:
                predicted_angle_info = f"Pentagon Class {angle_class}"
        elif label == 'rectangle':
            x1, y1, x2, y2 = original_bbox
            width = x2 - x1
            height = y2 - y1
            if abs(width - height) < 5:
                predicted_angle_info = "0 "
            elif height >= 2 * width:
                predicted_angle_info = "90 "
            else:
                predicted_angle_info = "0 "
        elif label == 'Racetrack':
            racetrack_angles = {0: "45 ", 1: "90 ", 2: "135 ", 3: "0 "}
            predicted_angle_info = racetrack_angles.get(angle_class, f"Racetrack Class {angle_class}")
        elif label == 'Triangle':
            if angle_class == 0:
                predicted_angle_info = "0 "
            elif angle_class == 2:
                predicted_angle_info = "180 "
            else:
                if "triangle_aux" in angle_models:
                    aux_class = predict_angle(cropped_pil_image, angle_models["triangle_aux"])
                    if aux_class == 0:
                        predicted_angle_info = "90  (rightward tip)"
                    elif aux_class == 1:
                        predicted_angle_info = "270  (leftward tip)"
                    else:
                        predicted_angle_info = f"Triangle Auxiliary Class {aux_class} (unknown direction)"
                else:
                    predicted_angle_info = "Triangle auxiliary weights not found."
        elif label in ['arrow']:
            arrow_angles = {0: "0 ", 1: "90 ", 2: "180 ", 3: "270 "}
            predicted_angle_info = arrow_angles.get(angle_class, f"{label.capitalize()} Class {angle_class}")
        else:
            if num_classes == 8:
                angle_class_to_degrees_8_classes = {
                    0: "0 ", 1: "45 ", 2: "90 ", 3: "135 ",
                    4: "180 ", 5: "225 ", 6: "270 ", 7: "315 "
                }
                predicted_angle_info = angle_class_to_degrees_8_classes.get(angle_class, f"Class {angle_class} (Unknown Degree)")
            elif num_classes == 4:
                generic_4_class_angles = {
                    0: "0 ", 1: "90 ", 2: "180 ", 3: "270 "
                }
                predicted_angle_info = generic_4_class_angles.get(angle_class, f"Class {angle_class} (Unknown Degree)")
            elif num_classes == 2:
                generic_2_class_angles = {
                    0: "0 ", 1: "180 "
                }
                predicted_angle_info = generic_2_class_angles.get(angle_class, f"Class {angle_class} (Unknown Degree)")
            else:
                predicted_angle_info = f"Class {angle_class}"
    elif angle_source == "none":
        print(f"No angle model available for '{label}' (Box {i}). Skipping angle prediction.")

    predicted_angles.append({
        "angle": predicted_angle_info,
        "source": angle_source
    })
    print(f"Predicted Angle for '{label}' (Box {i}): {predicted_angle_info} (source: {angle_source})")

# %% ../nbs/index.ipynb 66
#| eval: false
import cv2
import numpy as np

def get_corner_background_color(image):

    h, w = image.shape[:2]
    s = min(5, w // 2, h // 2)

    tl = image[0:s, 0:s].reshape(-1, 3)
    tr = image[0:s, w-s:w].reshape(-1, 3)
    bl = image[h-s:h, 0:s].reshape(-1, 3)
    br = image[h-s:h, w-s:w].reshape(-1, 3)

    corners = np.vstack([tl, tr, bl, br])
    mean_bgr = np.mean(corners, axis=0)
    return mean_bgr

def find_smart_dominant_color(image, bg_color_bgr, k=4):

    try:
        lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        pixels = lab_image.reshape((-1, 3)).astype(np.float32)

        if pixels.shape[0] < 10:
            return None

        K = min(k, max(1, pixels.shape[0] // 200))
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)
        _, labels, centers = cv2.kmeans(
            pixels, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS
        )

        counts = np.bincount(labels.flatten())

        bg_color_uint8 = np.array([[bg_color_bgr]], dtype=np.uint8)
        bg_lab = cv2.cvtColor(bg_color_uint8, cv2.COLOR_BGR2LAB)[0][0].astype(np.float32)

        sorted_indices = np.argsort(counts)[::-1]

        final_color_bgr = None

        for idx in sorted_indices:
            center_lab = centers[idx]

            dist = np.linalg.norm(center_lab - bg_lab)

            if dist > 15:
                color_lab = center_lab.astype("uint8").reshape(1, 1, 3)
                final_color_bgr = cv2.cvtColor(color_lab, cv2.COLOR_LAB2BGR).reshape(3)
                break

        if final_color_bgr is None:
            color_lab = centers[sorted_indices[0]].astype("uint8").reshape(1, 1, 3)
            final_color_bgr = cv2.cvtColor(color_lab, cv2.COLOR_LAB2BGR).reshape(3)

        return tuple(int(c) for c in final_color_bgr[::-1])
    except Exception as e:
        return None

def rgb_to_hex(rgb_tuple):
    if rgb_tuple is None:
        return None
    r, g, b = rgb_tuple
    return "#{:02x}{:02x}{:02x}".format(
        max(0, min(255, int(r))),
        max(0, min(255, int(g))),
        max(0, min(255, int(b)))
    )

def iou(boxA, boxB):
    ax1,ay1,ax2,ay2 = boxA
    bx1,by1,bx2,by2 = boxB
    inter_x1 = max(ax1,bx1); inter_y1 = max(ay1,by1)
    inter_x2 = min(ax2,bx2); inter_y2 = min(ay2,by2)
    inter_w = max(0, inter_x2 - inter_x1); inter_h = max(0, inter_y2 - inter_y1)
    inter_area = inter_w * inter_h
    areaA = max(0, ax2-ax1) * max(0, ay2-ay1)
    areaB = max(0, bx2-bx1) * max(0, by2-by1)
    denom = areaA + areaB - inter_area
    return inter_area/denom if denom > 0 else 0.0

def greedy_iou_match(detections, angle_source, predicted_angles, iou_threshold=0.4):
    if angle_source is None or predicted_angles is None:
        return {}

    S = min(len(angle_source), len(predicted_angles))
    if S == 0:
        return {}

    D = len(detections)
    det_labels, det_boxes = [], []
    for d in detections:
        det_labels.append(d[0])
        p1,p2 = d[1], d[2]
        det_boxes.append((int(round(p1[0])), int(round(p1[1])), int(round(p2[0])), int(round(p2[1]))))

    src_labels, src_boxes = [], []
    for j in range(S):
        s = angle_source[j]
        src_labels.append(s[0])
        sp1, sp2 = s[1], s[2]
        src_boxes.append((int(round(sp1[0])), int(round(sp1[1])), int(round(sp2[0])), int(round(sp2[1]))))

    iou_mat = np.zeros((D, S), dtype=float)
    for i in range(D):
        for j in range(S):
            if det_labels[i] != src_labels[j]:
                continue
            iou_mat[i, j] = iou(det_boxes[i], src_boxes[j])

    matches = {}
    used_det = set(); used_src = set()
    while True:
        if iou_mat.size == 0:
            break
        idx = np.unravel_index(np.argmax(iou_mat), iou_mat.shape)
        max_iou = iou_mat[idx]
        if max_iou < iou_threshold:
            break
        di, sj = int(idx[0]), int(idx[1])
        if di in used_det or sj in used_src:
            iou_mat[di, sj] = 0.0
            continue
        matches[di] = predicted_angles[sj]
        used_det.add(di); used_src.add(sj)
        iou_mat[di, :] = 0.0
        iou_mat[:, sj] = 0.0

    return matches

def process_image_and_detections_simple(image_path, detections, predicted_angles=None, angle_source=None, iou_threshold=0.4):
    img = cv2.imread(image_path)
    if img is None:
        raise FileNotFoundError(f"Image not found at path: {image_path}")
    H, W = img.shape[:2]

    thin_line_labels = {"dashed-arrow", "doted-arrow", "connector"}

    allowed_angle_labels = {"arrow", "rectangle", "Racetrack", "Pentagon", "Triangle"}

    matches = greedy_iou_match(detections, angle_source, predicted_angles, iou_threshold=iou_threshold)

    out = []
    for i, det in enumerate(detections):
        if len(det) >= 4:
            label, p1, p2, existing_color = det[0], det[1], det[2], det[3]
        else:
            label, p1, p2 = det[0], det[1], det[2]
            existing_color = None

        x1 = max(0, int(round(float(p1[0])))); y1 = max(0, int(round(float(p1[1]))))
        x2 = min(W, int(round(float(p2[0])))); y2 = min(H, int(round(float(p2[1]))))

        if x2 <= x1 or y2 <= y1:
            dom_hex = existing_color
        else:
            full_crop = img[y1:y2, x1:x2]

            if full_crop is None or full_crop.size == 0:
                dom_hex = existing_color
            else:

                bg_color_bgr = get_corner_background_color(full_crop)

                h_c, w_c = full_crop.shape[:2]
                margin_h = int(h_c * 0.20)
                margin_w = int(w_c * 0.20)

                if margin_h > 0 and margin_w > 0:
                    center_crop = full_crop[margin_h:h_c-margin_h, margin_w:w_c-margin_w]
                else:
                    center_crop = full_crop

                dom_rgb = find_smart_dominant_color(center_crop, bg_color_bgr)
                dom_hex = existing_color if existing_color else rgb_to_hex(dom_rgb)

        angle_val = None
        if label in allowed_angle_labels:
            if i in matches:
                angle_val = matches[i]
            elif predicted_angles is not None and i < len(predicted_angles):
                angle_val = predicted_angles[i]

        out.append((label, (x1, y1), (x2, y2), dom_hex, angle_val))

    return out

try:
    detections_input = detections
except NameError:
    try:
        detections_input = detections_list
    except NameError:
        raise RuntimeError("Set 'detections' or 'detections_list' before running this cell.")

preds = globals().get('predicted_angles', None)
src = globals().get('filtered_detections', None)

final_detections = process_image_and_detections_simple(file_path, detections_input, predicted_angles=preds, angle_source=src, iou_threshold=0.4)


print("detections = [")
for det in final_detections:
    label, (x1, y1), (x2, y2), color_hex, angle_info = det
    print(f"    ('{label}', ({x1}, {y1}), ({x2}, {y2}), {repr(color_hex)}, {repr(angle_info)}),")
print("]")

# %% ../nbs/index.ipynb 67
#| eval: false
import numpy as np
import json
from PIL import Image

def create_nodes_json(shape_detections):

    diagram_nodes = []
    for i, det in enumerate(shape_detections):
        label = det[0] if len(det) > 0 else ""
        top_left = det[1] if len(det) > 1 else (0, 0)
        bottom_right = det[2] if len(det) > 2 else (0, 0)
        color = det[3] if len(det) > 3 else None
        angle = det[4] if len(det) > 4 else None

        if angle is None:
            angle_val = ""
        elif isinstance(angle, dict):
            raw = angle.get("angle", "")
            if isinstance(raw, str):
                angle_val = raw.strip()
            else:
                angle_val = str(raw) if raw is not None else ""
        elif isinstance(angle, str):
            angle_val = angle.strip()
            if angle_val.lower() == "none":
                angle_val = ""
        else:
            # It's a number
            angle_val = str(angle)

        x1, y1 = top_left
        x2, y2 = bottom_right
        center_x = int(round((x1 + x2) / 2))
        center_y = int(round((y1 + y2) / 2))
        width = int(round(x2 - x1))
        height = int(round(y2 - y1))

        node = {
            "id": f"node{i+1}",
            "x": center_x,
            "y": center_y,
            "text": "",
            "shape": label,
            "color": color,
            "width": width,
            "height": height,
            "angle": angle_val
        }
        diagram_nodes.append(node)
    return diagram_nodes

def _det_to_center(det):
    if det is None:
        return None
    label = det[0] if len(det) > 0 else ""
    tl = det[1] if len(det) > 1 else (0, 0)
    br = det[2] if len(det) > 2 else (0, 0)
    cx = int(round((tl[0] + br[0]) / 2))
    cy = int(round((tl[1] + br[1]) / 2))
    return (label, cx, cy)

def _find_node_by_det(diagram_nodes, det, tol=8):
    if det is None:
        return None
    label, cx, cy = _det_to_center(det)
    for node in diagram_nodes:
        if node['shape'] == label:
            if abs(node['x'] - cx) <= tol and abs(node['y'] - cy) <= tol:
                return node['id']
    best = (None, 1e9)
    for node in diagram_nodes:
        if node['shape'] == label:
            dist = (node['x']-cx)**2 + (node['y']-cy)**2
            if dist < best[1]:
                best = (node['id'], dist)
    return best[0]

def create_edges_json(arrow_connections, diagram_nodes, all_detections, arrowhead_radius=50):
    diagram_edges = []
    arrow_head_centers = []
    for d in [dd for dd in all_detections if dd[0] == "arrow_head"]:
        tl = d[1] if len(d) > 1 else (0, 0)
        br = d[2] if len(d) > 2 else (0, 0)
        arrow_head_centers.append(((tl[0]+br[0])/2.0, (tl[1]+br[1])/2.0))

    for i, conn in enumerate(arrow_connections):
        tail_det = conn.get("tail_connected_to")
        head_det = conn.get("head_connected_to")
        src_id = _find_node_by_det(diagram_nodes, tail_det)
        tgt_id = _find_node_by_det(diagram_nodes, head_det)
        if not src_id or not tgt_id:
            continue

        head_pt = tuple(conn.get('head')) if conn.get('head') is not None else None
        tail_pt = tuple(conn.get('tail')) if conn.get('tail') is not None else None

        def _has_arrow_at(pt):
            if pt is None:
                return False
            for ah in arrow_head_centers:
                if np.linalg.norm(np.array(pt) - np.array(ah)) < arrowhead_radius:
                    return True
            return False

        has_at_tail = _has_arrow_at(tail_pt)
        has_at_head = _has_arrow_at(head_pt)

        startArrow = False
        endArrow = False

        if has_at_tail and has_at_head:
            startArrow = True; endArrow = True
        elif has_at_tail and not has_at_head:
            src_id, tgt_id = tgt_id, src_id; startArrow = False; endArrow = True
        elif has_at_head and not has_at_tail:
            startArrow = False; endArrow = True
        else:
            if head_pt and tail_pt:
                x_diff = head_pt[0] - tail_pt[0]
                y_diff = head_pt[1] - tail_pt[1]
                if abs(x_diff) > abs(y_diff):
                    if x_diff < 0:
                        src_id, tgt_id = tgt_id, src_id
                else:
                    if y_diff < 0:
                        src_id, tgt_id = tgt_id, src_id

        raw_label = conn.get("original_label") or conn.get("label") or ""
        line_style = raw_label.split('-')[0] if raw_label else "solid"

        edge = {
            "id": f"edge{i+1}",
            "source": src_id,
            "target": tgt_id,
            "lineStyle": line_style,
            "startArrow": bool(startArrow),
            "endArrow": bool(endArrow),
            "color": "#333333"
        }
        diagram_edges.append(edge)

    return diagram_edges

def map_ocr_to_nodes(diagram_nodes, processed_ocr_data, relaxation_pixels=15, max_area_ratio=80):

    nodes = [n.copy() for n in diagram_nodes]
    ocr_boxes = []
    if not processed_ocr_data:
        return nodes, []

    for ocr in processed_ocr_data:
        bbox = ocr.get('bbox')
        text = ocr.get('text', "")
        if not bbox or not text.strip():
            continue

        ocr_x1, ocr_y1, ocr_x2, ocr_y2 = bbox
        ocr_w = max(1, ocr_x2 - ocr_x1)
        ocr_h = max(1, ocr_y2 - ocr_y1)
        ocr_area = ocr_w * ocr_h

        candidates = []
        for idx, node in enumerate(nodes):
            cx, cy, w, h = node['x'], node['y'], node['width'], node['height']
            node_x1 = cx - (w // 2) - relaxation_pixels
            node_y1 = cy - (h // 2) - relaxation_pixels
            node_x2 = cx + (w // 2) + relaxation_pixels
            node_y2 = cy + (h // 2) + relaxation_pixels

            if (ocr_x1 >= node_x1 and ocr_y1 >= node_y1 and ocr_x2 <= node_x2 and ocr_y2 <= node_y2):
                node_area = max(1, w * h)
                area_ratio = node_area / float(ocr_area)
                dx = cx - (ocr_x1 + ocr_x2) / 2.0
                dy = cy - (ocr_y1 + ocr_y2) / 2.0
                center_dist2 = dx * dx + dy * dy
                candidates.append((idx, node_area, area_ratio, center_dist2))

        if candidates:
            filtered = [c for c in candidates if c[2] <= max_area_ratio]
            chosen = min(filtered if filtered else candidates, key=lambda t: (t[1], t[3]))
            best_idx = chosen[0]
            node = nodes[best_idx]
            node['text'] = (node.get('text', "") + (" " if node.get('text') else "") + text).strip()
        else:
            ocr_boxes.append({
                "text": text,
                "x1": int(ocr_x1), "y1": int(ocr_y1),
                "x2": int(ocr_x2), "y2": int(ocr_y2)
            })

    standalone_text_labels = []
    merge_y_thresh = 18
    merge_x_thresh = 40
    if ocr_boxes:
        ocr_boxes = sorted(ocr_boxes, key=lambda b: (b['y1'], b['x1']))
        used = [False] * len(ocr_boxes)
        for i, box in enumerate(ocr_boxes):
            if used[i]:
                continue
            texts = [box['text'].strip()]
            x1, y1, x2, y2 = box['x1'], box['y1'], box['x2'], box['y2']
            used[i] = True
            for j in range(i + 1, len(ocr_boxes)):
                if used[j]:
                    continue
                ob = ocr_boxes[j]
                ob_x1, ob_y1, ob_x2, ob_y2 = ob['x1'], ob['y1'], ob['x2'], ob['y2']
                vertical_close = abs(ob_y1 - y2) <= merge_y_thresh or abs(ob_y2 - y1) <= merge_y_thresh
                horizontal_overlap = (ob_x1 <= x2 + merge_x_thresh and ob_x2 >= x1 - merge_x_thresh)
                if vertical_close and horizontal_overlap:
                    texts.append(ob['text'].strip())
                    x1 = min(x1, ob_x1)
                    y1 = min(y1, ob_y1)
                    x2 = max(x2, ob_x2)
                    y2 = max(y2, ob_y2)
                    used[j] = True

            standalone_text_labels.append({
                "id": f"text{len(standalone_text_labels) + 1}",
                "x": int(round((x1 + x2) / 2)),
                "y": int(round((y1 + y2) / 2)),
                "text": " ".join(texts).strip(),
                "bbox": {"x1": x1, "y1": y1, "x2": x2, "y2": y2},
                "width": x2 - x1,
                "height": y2 - y1
            })

    return nodes, standalone_text_labels


# ---- Build final JSON ----
source_detections = None
if 'final_detections' in globals() and isinstance(globals()['final_detections'], list):
    source_detections = globals()['final_detections']
elif 'final_detections' in globals() and globals()['final_detections'] is None:
    source_detections = None
elif 'detections' in globals() and isinstance(globals()['detections'], list):
    source_detections = globals()['detections']
elif 'detections_list' in globals() and isinstance(globals()['detections_list'], list):
    source_detections = globals()['detections_list']
else:
    raise RuntimeError("No detections available. Ensure `final_detections` or `detections` is defined in scope.")

shape_detections = [d for d in source_detections if d[0] not in ["dashed-arrow", "dotted-arrow", "solid-arrow", "arrow_head"]]
nodes_json = create_nodes_json(shape_detections)

if 'arrow_connections' not in globals():
    arrow_connections = []
edges_json = create_edges_json(arrow_connections, nodes_json, source_detections)

standalone_text_labels = []
if 'final_processed_data' in globals() and final_processed_data:
    nodes_json, standalone_text_labels = map_ocr_to_nodes(
        nodes_json, final_processed_data, relaxation_pixels=15,
)

if 'image' not in globals():
    raise RuntimeError("`image` not found in scope. Provide `image` (PIL Image or numpy array).")
img_obj = globals()['image']
if hasattr(img_obj, "shape"):
    canvas_w, canvas_h = int(img_obj.shape[1]), int(img_obj.shape[0])
else:
    try:
        canvas_w, canvas_h = int(img_obj.size[0]), int(img_obj.size[1])
    except Exception as e:
        raise RuntimeError("Cannot determine canvas size from `image`. Provide a numpy array or PIL Image.") from e

final_json_output = {
    "canvas": {"width": canvas_w, "height": canvas_h},
    "nodes": nodes_json,
    "edges": edges_json,
    "text_labels": standalone_text_labels
}

print("\n--- Final JSON Output ---")
print(json.dumps(final_json_output, indent=2))

# %% ../nbs/index.ipynb 69
#| eval: false
import json
import matplotlib.pyplot as plt


json_data = final_json_output

# --- Parse JSON ---
data = json_data
nodes = {node["id"]: node for node in data["nodes"]}
edges = data["edges"]
# --- Setup Canvas ---
fig, ax = plt.subplots(figsize=(data["canvas"]["width"]/100, data["canvas"]["height"]/100))
ax.set_xlim(0, data["canvas"]["width"])
ax.set_ylim(0, data["canvas"]["height"])
ax.invert_yaxis()
ax.axis("off")

for node in nodes.values():
    label = node["text"] if node["text"] else node["shape"]
    ax.text(node["x"], node["y"], label, ha="center", va="center", fontsize=8, color="black", wrap=True)
for edge in edges:
    src = nodes[edge["source"]]
    tgt = nodes[edge["target"]]
    x1, y1 = src["x"], src["y"]
    x2, y2 = tgt["x"], tgt["y"]
    ax.annotate("",
                xy=(x2, y2), xytext=(x1, y1),
                arrowprops=dict(arrowstyle="->", color="gray", lw=1.2))
plt.tight_layout()
plt.show()

# %% ../nbs/index.ipynb 71
#| eval: false
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def clean_and_extract_number(value):

    if pd.isna(value):
        return 0
    s_value = str(value).strip()
    parts = s_value.split(' ', 1)
    numeric_part = parts[0]

    try:
        return int(float(numeric_part))
    except (ValueError, TypeError):
        return 0

def create_confusion_matrix(file_path):

    try:
        df = pd.read_excel(file_path)
        df.columns = df.columns.str.strip()

        numeric_columns = ['TOTAL ARROW', 'CORECT ARROW NUMBER', 'WRONG ARROW NUMBER']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = df[col].apply(clean_and_extract_number)
            else:
                print(f"Warning: Column '{col}' not found. Please check spelling in your Excel file.")

        true_positives = df['CORECT ARROW NUMBER'].sum()
        false_positives = df['WRONG ARROW NUMBER'].sum()
        false_negatives = (df['TOTAL ARROW'] - df['CORECT ARROW NUMBER']).sum()

        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0

        print("----------------------------------------")
        print("   Algorithm Performance Metrics")
        print("----------------------------------------")
        print(f"True Positives (TP):  {true_positives}")
        print(f"False Positives (FP): {false_positives}")
        print(f"False Negatives (FN): {false_negatives}")
        print("True Negatives (TN):  Zero 0.")
        print("\n--- Key Metrics ---")
        print(f"Precision: {precision:.2%}")
        print(f"Recall (Sensitivity): {recall:.2%}")
        print("----------------------------------------\n")

        matrix_data = [[true_positives, false_negatives], [false_positives, 0]]

        matrix_labels = [
            [f"{true_positives}\n(TP)", f"{false_negatives}\n(FN)"],
            [f"{false_positives}\n(FP)", "Zero (0) \n(TN)"]
        ]

        plt.figure(figsize=(8, 6))
        sns.heatmap(matrix_data, annot=matrix_labels, fmt="", cmap='Blues',
                    xticklabels=['Predicted: Arrow', 'Predicted: Not Arrow'],
                    yticklabels=['Actual: Arrow', 'Actual: Not Arrow'],
                    cbar=False, annot_kws={"size": 14})

        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.title('Confusion Matrix for Arrow Detection', fontsize=16)

        # This command displays the plot
        plt.show()

    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == '__main__':
    file_path = '/content/REPORT.xlsx'
    create_confusion_matrix(file_path)

# %% ../nbs/index.ipynb 72
import json
import math
import textwrap
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont

class DiagramGenerator:
    def __init__(self, json_data):
        self.data = json_data
        self.canvas_w = json_data['canvas']['width']
        self.canvas_h = json_data['canvas']['height']
        self.image = Image.new('RGB', (self.canvas_w, self.canvas_h), color='white')
        self.draw = ImageDraw.Draw(self.image)

        try:
            self.font = ImageFont.truetype("arial.ttf", 14)
            self.font_bold = ImageFont.truetype("arialbd.ttf", 13)
        except IOError:
            self.font = ImageFont.load_default()
            self.font_bold = ImageFont.load_default()

    def generate_image(self):
        nodes = self.data.get('nodes', [])


        sorted_nodes = sorted(nodes, key=lambda n: n['width'] * n['height'], reverse=True)

        for node in sorted_nodes:
            self.draw_node(node)

        for label in self.data.get('text_labels', []):
            self.draw_label(label)

        return self.image

    def get_bbox(self, node):
        cx, cy = node['x'], node['y']
        w, h = node['width'], node['height']
        return (cx - w/2, cy - h/2, cx + w/2, cy + h/2)

    def get_poly_points(self, cx, cy, w, h, shape_type):
        if shape_type == "diamond":
            return [(0, -h/2), (w/2, 0), (0, h/2), (-w/2, 0)]
        elif shape_type == "triangle":
            return [(0, -h/2), (w/2, h/2), (-w/2, h/2)]
        elif shape_type == "pentagon":
            return self._calculate_ngon(5, w/2, h/2)
        elif "arrow" in shape_type and "double" not in shape_type:
            head_len = w * 0.4
            return [(-w/2, -h/4), (w/2 - head_len, -h/4), (w/2 - head_len, -h/2),
                    (w/2, 0), (w/2 - head_len, h/2), (w/2 - head_len, h/4), (-w/2, h/4)]
        elif "double arrow" in shape_type:
            head_w = w * 0.2
            shaft_h = h * 0.4
            return [(-w/2, 0), (-w/2 + head_w, -h/2), (-w/2 + head_w, -shaft_h/2),
                    (w/2 - head_w, -shaft_h/2), (w/2 - head_w, -h/2), (w/2, 0),
                    (w/2 - head_w, h/2), (w/2 - head_w, shaft_h/2),
                    (-w/2 + head_w, shaft_h/2), (-w/2 + head_w, h/2)]
        return [(-w/2, -h/2), (w/2, -h/2), (w/2, h/2), (-w/2, h/2)]

    def draw_node(self, node):
        raw_shape = node.get('shape', 'rectangle')
        shape_type = raw_shape.lower().replace("_", " ").strip()

        cx, cy = node['x'], node['y']
        w, h = node['width'], node['height']

        color = node.get('color', '#cccccc')
        if len(color) == 9 and color.startswith('#'): color = color[:7]

        try: angle = float(node.get('angle', 0))
        except: angle = 0

        text = node.get('text', '')

        x1, y1, x2, y2 = self.get_bbox(node)

        if shape_type in ["circle", "ellipse", "start", "end"]:
            self.draw.ellipse([x1, y1, x2, y2], fill=color)
        elif shape_type == "racetrack" or shape_type == "terminal":
            self.draw.rounded_rectangle([x1, y1, x2, y2], radius=h/2, fill=color)
        elif shape_type in ["rounded rectangle", "process"]:
            self.draw.rounded_rectangle([x1, y1, x2, y2], radius=15, fill=color)
        elif shape_type == "cloud":
            self.draw.ellipse([x1, y1, x2, y2], fill=color)
        elif shape_type == "star":
            points = self._calculate_star_points(5, w/2, w/4)
            self._draw_rotated_polygon(points, cx, cy, angle, color)
        else:
            points = self.get_poly_points(cx, cy, w, h, shape_type)
            self._draw_rotated_polygon(points, cx, cy, angle, color)

        if text:
            self._draw_text_centered(text, cx, cy, w, h)

    def draw_label(self, label):
        self.draw.text((label['x'], label['y']), label.get('text', ''), font=self.font, fill="black")

    def _rotate_point(self, point, angle_deg):
        angle_rad = math.radians(angle_deg)
        x, y = point
        return (x * math.cos(angle_rad) - y * math.sin(angle_rad),
                x * math.sin(angle_rad) + y * math.cos(angle_rad))

    def _draw_rotated_polygon(self, points, cx, cy, angle, color):
        rotated = []
        for p in points:
            rx, ry = self._rotate_point(p, angle)
            rotated.append((cx + rx, cy + ry))
        self.draw.polygon(rotated, fill=color)

    def _calculate_ngon(self, sides, radius_x, radius_y):
        points = []
        for i in range(sides):
            angle = (2 * math.pi * i) / sides - (math.pi / 2)
            points.append((math.cos(angle) * radius_x, math.sin(angle) * radius_y))
        return points

    def _calculate_star_points(self, points_count, outer_radius, inner_radius):
        points = []
        angle_step = math.pi / points_count
        current_angle = -math.pi / 2
        for i in range(points_count * 2):
            radius = outer_radius if i % 2 == 0 else inner_radius
            points.append((math.cos(current_angle) * radius, math.sin(current_angle) * radius))
            current_angle += angle_step
        return points

    def _draw_text_centered(self, text, cx, cy, w, h):
        if not text: return

        char_w_approx = 6.5
        padding = 10
        max_chars = max(1, int((w - padding) / char_w_approx))

        lines = textwrap.wrap(text, width=max_chars)

        line_height = 14
        total_text_h = len(lines) * line_height

        current_y = cy - (total_text_h / 2)

        for line in lines:
            bbox = self.draw.textbbox((0, 0), line, font=self.font_bold)
            text_w = bbox[2] - bbox[0]
            text_x = cx - (text_w / 2)
            self.draw.text((text_x, current_y), line, font=self.font_bold, fill="black")
            current_y += line_height

if __name__ == "__main__":

    data = {
  "canvas": {
    "width": 801,
    "height": 798
  },
  "nodes": [
    {
      "id": "node1",
      "x": 649,
      "y": 534,
      "text": "Hope is a dangerous illusion",
      "shape": "Racetrack",
      "color": "#5f803e",
      "width": 248,
      "height": 395,
      "angle": {
        "angle": "290.7 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node2",
      "x": 143,
      "y": 420,
      "text": "#5",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 136,
      "height": 132,
      "angle": ""
    },
    {
      "id": "node3",
      "x": 414,
      "y": 400,
      "text": "Fragments of Fire: Short Truths for a Heavy Mind",
      "shape": "Pentagon",
      "color": "#263619",
      "width": 431,
      "height": 415,
      "angle": {
        "angle": "0.0 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node4",
      "x": 566,
      "y": 197,
      "text": "The void always stares back.",
      "shape": "Racetrack",
      "color": "#faf9e7",
      "width": 335,
      "height": 320,
      "angle": {
        "angle": "41.2 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node5",
      "x": 260,
      "y": 692,
      "text": "#4",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 137,
      "height": 133,
      "angle": ""
    },
    {
      "id": "node6",
      "x": 608,
      "y": 634,
      "text": "#3",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 136,
      "height": 133,
      "angle": ""
    },
    {
      "id": "node7",
      "x": 266,
      "y": 194,
      "text": "We rot beautifully.",
      "shape": "Racetrack",
      "color": "#6d7a4b",
      "width": 351,
      "height": 317,
      "angle": {
        "angle": "0.0 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node8",
      "x": 648,
      "y": 264,
      "text": "#2",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 135,
      "height": 133,
      "angle": ""
    },
    {
      "id": "node9",
      "x": 356,
      "y": 130,
      "text": "#1",
      "shape": "circle",
      "color": "#ba6c26",
      "width": 133,
      "height": 132,
      "angle": ""
    },
    {
      "id": "node10",
      "x": 378,
      "y": 692,
      "text": "Light lies; shadows remember.",
      "shape": "Racetrack",
      "color": "#dcaa73",
      "width": 391,
      "height": 163,
      "angle": {
        "angle": "2.5 ",
        "source": "ocr_text"
      }
    },
    {
      "id": "node11",
      "x": 182,
      "y": 538,
      "text": "Especially you. Everything lends.",
      "shape": "Racetrack",
      "color": "#c8945a",
      "width": 249,
      "height": 399,
      "angle": {
        "angle": "178.0 ",
        "source": "ocr_text"
      }
    }
  ],
  "edges": [],
  "text_labels": []
}

    gen = DiagramGenerator(data)
    final_img = gen.generate_image()

    plt.figure(figsize=(10, 8))
    plt.imshow(final_img)
    plt.axis('off')
    plt.show()

# %% ../nbs/index.ipynb 73
#| export
